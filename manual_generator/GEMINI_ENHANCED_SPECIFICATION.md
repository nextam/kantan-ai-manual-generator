# Manual Generator - Gemini 2.5 Proçµ±åˆä»•æ§˜æ›¸

## ğŸ“‹ æ¦‚è¦

Gemini 2.5 Proã‚’ä¸­æ ¸AIã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦æ´»ç”¨ã—ã€è£½é€ æ¥­å‘ã‘ã®é«˜åº¦ãªä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã€‚å‹•ä½œåˆ†æã€æ–‡æ›¸å‡¦ç†ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€OCRå‡¦ç†ã¾ã§ã€å…¨ã¦ã‚’Geminiã®å¤šæ©Ÿèƒ½æ€§ã§çµ±åˆå®Ÿç¾ã€‚

## ğŸ¤– Gemini 2.5 Proæ´»ç”¨æˆ¦ç•¥

### 1. çµ±åˆAIæ©Ÿèƒ½ãƒãƒƒãƒ”ãƒ³ã‚°

```python
# Gemini 2.5 Proæ©Ÿèƒ½çµ±åˆè¨­è¨ˆ
GEMINI_FUNCTIONS = {
    "video_analysis": {
        "model": "gemini-2.5-pro",
        "capabilities": [
            "multimodal_understanding",  # å‹•ç”»ãƒ»ç”»åƒç†è§£
            "motion_tracking",           # å‹•ä½œè¿½è·¡åˆ†æ
            "object_detection",          # ç‰©ä½“èªè­˜
            "temporal_analysis"          # æ™‚ç³»åˆ—åˆ†æ
        ]
    },
    "document_processing": {
        "model": "gemini-2.5-pro", 
        "capabilities": [
            "ocr_processing",            # OCRãƒ»æ–‡å­—èªè­˜
            "document_understanding",    # æ–‡æ›¸æ§‹é€ ç†è§£
            "table_extraction",          # è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            "semantic_search"            # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        ]
    },
    "content_generation": {
        "model": "gemini-2.5-pro",
        "capabilities": [
            "manual_creation",           # ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ
            "comparison_analysis",       # æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
            "terminology_explanation",  # å°‚é–€ç”¨èªè§£èª¬
            "safety_recommendations"    # å®‰å…¨æ€§ææ¡ˆ
        ]
    },
    "function_calling": {
        "model": "gemini-2.5-pro",
        "capabilities": [
            "workflow_orchestration",   # å‡¦ç†ãƒ•ãƒ­ãƒ¼åˆ¶å¾¡
            "data_extraction",           # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            "validation_checking",       # å“è³ªæ¤œè¨¼
            "recommendation_engine"      # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
        ]
    }
}
```

## ğŸ—ï¸ Geminiçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 2. çµ±åˆAIã‚µãƒ¼ãƒ“ã‚¹å±¤

```python
# modules/gemini_service.py
import google.generativeai as genai
import vertexai
from vertexai.generative_models import GenerativeModel, Part
from typing import List, Dict, Any, Optional
import json
import base64

class GeminiUnifiedService:
    """Gemini 2.5 Proçµ±åˆã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # Vertex AIåˆæœŸåŒ–
        vertexai.init(project=PROJECT_ID, location='us-central1')
        self.model = GenerativeModel('gemini-2.5-pro')
        
        # Function Callingå®šç¾©
        self.functions = self._setup_function_definitions()
    
    def _setup_function_definitions(self):
        """Gemini Function Callingç”¨é–¢æ•°å®šç¾©"""
        return [
            {
                "name": "extract_work_steps",
                "description": "å‹•ç”»ã‹ã‚‰ä½œæ¥­æ‰‹é †ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡º",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "step_number": {"type": "integer"},
                                    "action": {"type": "string"},
                                    "tools_used": {"type": "array", "items": {"type": "string"}},
                                    "duration": {"type": "number"},
                                    "safety_notes": {"type": "string"},
                                    "quality_points": {"type": "string"}
                                }
                            }
                        }
                    }
                }
            },
            {
                "name": "compare_work_techniques",
                "description": "ç†Ÿç·´è€…ã¨éç†Ÿç·´è€…ã®ä½œæ¥­æŠ€è¡“ã‚’æ¯”è¼ƒåˆ†æ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "differences": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "aspect": {"type": "string"},
                                    "expert_approach": {"type": "string"},
                                    "novice_approach": {"type": "string"},
                                    "improvement_suggestion": {"type": "string"},
                                    "impact_level": {"type": "string", "enum": ["high", "medium", "low"]}
                                }
                            }
                        }
                    }
                }
            },
            {
                "name": "extract_document_data",
                "description": "æŠ€è¡“æ–‡æ›¸ã‹ã‚‰ã‚­ãƒ¼æƒ…å ±ã‚’æŠ½å‡º",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "document_type": {"type": "string"},
                        "key_data": {"type": "object"},
                        "terminology": {"type": "array", "items": {"type": "object"}},
                        "related_procedures": {"type": "array", "items": {"type": "string"}}
                    }
                }
            }
        ]
    
    async def analyze_expert_novice_comparison(self, expert_video: str, novice_video: str, context_docs: List[str] = None):
        """ç†Ÿç·´è€…ãƒ»éç†Ÿç·´è€…å‹•ç”»ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ"""
        
        # 1. å€‹åˆ¥å‹•ç”»åˆ†æ
        expert_analysis = await self._analyze_single_video(expert_video, "expert")
        novice_analysis = await self._analyze_single_video(novice_video, "novice")
        
        # 2. æ–‡æ›¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
        doc_context = ""
        if context_docs:
            doc_context = await self._process_context_documents(context_docs)
        
        # 3. æ¯”è¼ƒåˆ†æå®Ÿè¡Œ
        comparison_prompt = f"""
        è£½é€ æ¥­ã®ä½œæ¥­å‹•ç”»æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

        # åˆ†æå¯¾è±¡
        ç†Ÿç·´è€…å‹•ç”»åˆ†æçµæœ:
        {expert_analysis}
        
        éç†Ÿç·´è€…å‹•ç”»åˆ†æçµæœ:
        {novice_analysis}
        
        # å‚è€ƒè³‡æ–™
        {doc_context}
        
        # åˆ†æè¦æ±‚
        ä»¥ä¸‹ã®è¦³ç‚¹ã§è©³ç´°ãªæ¯”è¼ƒåˆ†æã‚’è¡Œã„ã€compare_work_techniquesé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
        
        1. å‹•ä½œåŠ¹ç‡æ€§ã®é•ã„
        2. å®‰å…¨æ€§ã¸ã®é…æ…®ã®å·®
        3. å“è³ªç®¡ç†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„
        4. å·¥å…·ä½¿ç”¨æ–¹æ³•ã®å·®ç•°
        5. æ™‚é–“åŠ¹ç‡æ€§ã®æ¯”è¼ƒ
        6. æ”¹å–„ææ¡ˆã®å„ªå…ˆé †ä½ä»˜ã‘
        """
        
        response = await self.model.generate_content_async(
            comparison_prompt,
            tools=[{"function_declarations": self.functions}],
            generation_config={
                "temperature": 0.1,  # åˆ†æã®ä¸€è²«æ€§ã‚’é‡è¦–
                "top_p": 0.8,
                "max_output_tokens": 8192
            }
        )
        
        return self._parse_function_call_response(response)
    
    async def _analyze_single_video(self, video_uri: str, skill_level: str):
        """å˜ä¸€å‹•ç”»ã®è©³ç´°åˆ†æ"""
        
        video_part = Part.from_uri(video_uri, mime_type='video/mp4')
        
        analysis_prompt = f"""
        ã“ã®å‹•ç”»ã¯è£½é€ æ¥­ã«ãŠã‘ã‚‹{skill_level}ï¼ˆç†Ÿç·´è€…/éç†Ÿç·´è€…ï¼‰ã®ä½œæ¥­æ˜ åƒã§ã™ã€‚
        
        ä»¥ä¸‹ã®è¦³ç‚¹ã§è©³ç´°åˆ†æã‚’è¡Œã„ã€extract_work_stepsé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ï¼š
        
        1. ä½œæ¥­æ‰‹é †ã®åˆ†è§£ã¨æ™‚ç³»åˆ—åˆ†æ
        2. ä½¿ç”¨å·¥å…·ã®è­˜åˆ¥ã¨ä½¿ç”¨æ–¹æ³•
        3. å®‰å…¨æ€§ã«é–¢ã™ã‚‹è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
        4. å“è³ªç®¡ç†ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        5. ç„¡é§„ãªå‹•ä½œã‚„åŠ¹ç‡çš„ãªå‹•ä½œã®è­˜åˆ¥
        6. æ³¨æ„ã™ã¹ãå±é™ºç®‡æ‰€
        
        ç‰¹ã«{skill_level}ç‰¹æœ‰ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æ³¨ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚
        """
        
        response = await self.model.generate_content_async(
            [video_part, analysis_prompt],
            tools=[{"function_declarations": self.functions}],
            generation_config={
                "temperature": 0.1,
                "top_p": 0.8,
                "max_output_tokens": 8192
            }
        )
        
        return self._parse_function_call_response(response)
    
    async def process_document_with_ocr(self, document_path: str, document_type: str):
        """æ–‡æ›¸ã®OCRå‡¦ç†ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        
        # ç”»åƒã¨ã—ã¦æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿
        with open(document_path, 'rb') as f:
            document_data = base64.b64encode(f.read()).decode()
        
        document_part = Part.from_data(
            data=base64.b64decode(document_data),
            mime_type='image/jpeg'  # é©åˆ‡ãªMIMEã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
        )
        
        ocr_prompt = f"""
        ã“ã®{document_type}ï¼ˆè¦‹ç©æ›¸/å›³é¢/ä¸å…·åˆå ±å‘Šæ›¸ãªã©ï¼‰ã®ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
        
        1. å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’OCRã§æ­£ç¢ºã«èª­ã¿å–ã‚Š
        2. æ–‡æ›¸ã®æ§‹é€ ã¨éšå±¤ã‚’ç†è§£
        3. ã‚­ãƒ¼æƒ…å ±ã‚’æŠ½å‡ºï¼ˆéƒ¨å“ç•ªå·ã€ä»•æ§˜ã€æ³¨æ„äº‹é …ãªã©ï¼‰
        4. å°‚é–€ç”¨èªã¨ãã®å®šç¾©ã‚’è­˜åˆ¥
        5. ä½œæ¥­æ‰‹é †ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡º
        
        extract_document_dataé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        """
        
        response = await self.model.generate_content_async(
            [document_part, ocr_prompt],
            tools=[{"function_declarations": self.functions}],
            generation_config={
                "temperature": 0.1,
                "top_p": 0.8,
                "max_output_tokens": 8192
            }
        )
        
        return self._parse_function_call_response(response)
    
    async def generate_comprehensive_manual(self, analysis_data: Dict, output_config: Dict):
        """åŒ…æ‹¬çš„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ"""
        
        generation_prompt = f"""
        è£½é€ æ¥­ä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®è‡ªå‹•ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        åˆ†æçµæœ: {json.dumps(analysis_data, ensure_ascii=False, indent=2)}
        
        # å‡ºåŠ›è¨­å®š
        {json.dumps(output_config, ensure_ascii=False, indent=2)}
        
        # ç”Ÿæˆè¦æ±‚
        ä»¥ä¸‹ã®æ§‹é€ ã§é«˜å“è³ªãªãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
        
        ## 1. ä½œæ¥­æ¦‚è¦
        - ç›®çš„ã¨é‡è¦æ€§
        - å¿…è¦ãªæŠ€èƒ½ãƒ¬ãƒ™ãƒ«
        - æ¨å®šä½œæ¥­æ™‚é–“
        
        ## 2. æº–å‚™å·¥ç¨‹
        - å¿…è¦å·¥å…·ä¸€è¦§
        - ææ–™ãƒ»éƒ¨å“ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        - å®‰å…¨è£…å‚™ç¢ºèª
        
        ## 3. è©³ç´°ä½œæ¥­æ‰‹é †
        - ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—æŒ‡ç¤º
        - å„å·¥ç¨‹ã®å“è³ªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        - æ³¨æ„äº‹é …ã¨å®‰å…¨è­¦å‘Š
        
        ## 4. ç†Ÿç·´è€…ã®ã‚³ãƒ„
        - åŠ¹ç‡åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ
        - ã‚ˆãã‚ã‚‹å¤±æ•—ã¨å¯¾ç­–
        - å“è³ªå‘ä¸Šã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
        
        ## 5. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        - ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•
        - ç·Šæ€¥æ™‚ã®å¯¾å¿œæ‰‹é †
        
        æ–‡ä½“ã¯{output_config.get('writing_style', 'formal')}ã§ã€
        è©³ç´°åº¦ã¯{output_config.get('content_length', 'normal')}ãƒ¬ãƒ™ãƒ«ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        """
        
        response = await self.model.generate_content_async(
            generation_prompt,
            generation_config={
                "temperature": 0.3,  # å‰µé€ æ€§ã¨ãƒãƒ©ãƒ³ã‚¹
                "top_p": 0.9,
                "max_output_tokens": 65535  # Gemini 2.5 Proã®æœ€å¤§æ´»ç”¨
            }
        )
        
        return response.text
    
    async def extract_key_frames_with_ai(self, video_uri: str, manual_content: str):
        """AIã«ã‚ˆã‚‹é‡è¦ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        video_part = Part.from_uri(video_uri, mime_type='video/mp4')
        
        frame_extraction_prompt = f"""
        ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹ã¨å‹•ç”»ã‚’ç…§åˆã—ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
        
        ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹:
        {manual_content}
        
        # å®Ÿè¡Œå†…å®¹
        1. ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®å„å·¥ç¨‹ã«å¯¾å¿œã™ã‚‹é‡è¦ãªç¬é–“ã‚’å‹•ç”»ã‹ã‚‰ç‰¹å®š
        2. å®‰å…¨æ€§ã‚„å“è³ªã«é–¢ã‚ã‚‹é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’ç”»åƒã¨ã—ã¦æŠ½å‡º
        3. å„ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾ã™ã‚‹è©³ç´°ãªèª¬æ˜ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ç”Ÿæˆ
        4. æ¨å¥¨ã™ã‚‹ç”»åƒæŒ¿å…¥ä½ç½®ã‚’ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…ã§ç‰¹å®š
        
        # å‡ºåŠ›å½¢å¼
        ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæŒ‡ç¤ºã¨è©³ç´°èª¬æ˜ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        """
        
        response = await self.model.generate_content_async(
            [video_part, frame_extraction_prompt],
            generation_config={
                "temperature": 0.2,
                "top_p": 0.8,
                "max_output_tokens": 8192
            }
        )
        
        return json.loads(response.text)
    
    def _parse_function_call_response(self, response):
        """Function Calling ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹"""
        if response.candidates[0].content.parts[0].function_call:
            function_call = response.candidates[0].content.parts[0].function_call
            return {
                "function_name": function_call.name,
                "arguments": dict(function_call.args)
            }
        else:
            return {"text_response": response.text}
```

### 3. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµ±åˆ

```python
# app.py ã¸ã®è¿½åŠ æ©Ÿèƒ½
from modules.gemini_service import GeminiUnifiedService

@app.route('/ai_comparison_analysis', methods=['POST'])
async def ai_comparison_analysis():
    """Gemini AIã«ã‚ˆã‚‹ç†Ÿç·´è€…ãƒ»éç†Ÿç·´è€…æ¯”è¼ƒåˆ†æ"""
    try:
        data = request.get_json()
        gemini_service = GeminiUnifiedService()
        
        result = await gemini_service.analyze_expert_novice_comparison(
            expert_video=data['expert_video_uri'],
            novice_video=data['novice_video_uri'],
            context_docs=data.get('reference_documents', [])
        )
        
        return jsonify({
            'success': True,
            'analysis_result': result,
            'ai_engine': 'gemini-2.5-pro'
        })
        
    except Exception as e:
        return jsonify({'error': f'AIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/ai_document_processing', methods=['POST'])
async def ai_document_processing():
    """Gemini AIã«ã‚ˆã‚‹æ–‡æ›¸å‡¦ç†ãƒ»OCR"""
    try:
        files = request.files.getlist('documents')
        gemini_service = GeminiUnifiedService()
        
        processed_docs = []
        for file in files:
            # ä¸€æ™‚ä¿å­˜
            temp_path = save_temp_file(file)
            
            # Gemini OCR & æ§‹é€ åŒ–
            result = await gemini_service.process_document_with_ocr(
                temp_path, 
                file.filename.split('.')[-1]
            )
            
            processed_docs.append(result)
        
        return jsonify({
            'success': True,
            'processed_documents': processed_docs,
            'ai_engine': 'gemini-2.5-pro'
        })
        
    except Exception as e:
        return jsonify({'error': f'æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/ai_comprehensive_manual_generation', methods=['POST'])
async def ai_comprehensive_manual_generation():
    """Gemini AIã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ"""
    try:
        data = request.get_json()
        gemini_service = GeminiUnifiedService()
        
        # 1. æ¯”è¼ƒåˆ†æå®Ÿè¡Œ
        if 'expert_video' in data and 'novice_video' in data:
            comparison_result = await gemini_service.analyze_expert_novice_comparison(
                data['expert_video'], 
                data['novice_video'],
                data.get('reference_documents')
            )
        else:
            comparison_result = {}
        
        # 2. ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ
        manual_content = await gemini_service.generate_comprehensive_manual(
            comparison_result,
            data['output_config']
        )
        
        # 3. ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        if data.get('include_images', True):
            key_frames = await gemini_service.extract_key_frames_with_ai(
                data['expert_video'],
                manual_content
            )
        else:
            key_frames = []
        
        return jsonify({
            'success': True,
            'manual_content': manual_content,
            'key_frames': key_frames,
            'comparison_analysis': comparison_result,
            'ai_engine': 'gemini-2.5-pro',
            'generation_timestamp': time.time()
        })
        
    except Exception as e:
        return jsonify({'error': f'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500
```

## ğŸ¯ Gemini 2.5 Proæ´»ç”¨ã®åˆ©ç‚¹

### 1. çµ±åˆAIå‡¦ç†ã«ã‚ˆã‚‹é«˜ç²¾åº¦åŒ–
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç†è§£**: å‹•ç”»ãƒ»ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆã®åŒæ™‚å‡¦ç†
- **æ–‡è„ˆç†è§£**: å°‚é–€ç”¨èªã¨ä½œæ¥­æ‰‹é †ã®é–¢é€£æ€§ç†è§£
- **ä¸€è²«æ€§ç¢ºä¿**: å˜ä¸€AIã«ã‚ˆã‚‹å‡¦ç†ã§ã®æƒ…å ±æ•´åˆæ€§

### 2. Function Callingã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡ºåŠ›
- **ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š**: æ±ºã‚ã‚‰ã‚ŒãŸå½¢å¼ã§ã®ç¢ºå®Ÿãªå‡ºåŠ›
- **å‡¦ç†åŠ¹ç‡åŒ–**: å¾Œå‡¦ç†ã®è‡ªå‹•åŒ–ã¨ã‚¨ãƒ©ãƒ¼å‰Šæ¸›
- **æ‹¡å¼µæ€§**: æ–°æ©Ÿèƒ½è¿½åŠ æ™‚ã®é–¢æ•°å®šç¾©æ‹¡å¼µ

### 3. å¤§å®¹é‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ´»ç”¨
- **åŒ…æ‹¬çš„åˆ†æ**: è¤‡æ•°å‹•ç”»ãƒ»æ–‡æ›¸ã®åŒæ™‚è€ƒæ…®
- **è©³ç´°å‡ºåŠ›**: 65,535ãƒˆãƒ¼ã‚¯ãƒ³ã§ã®è©³ç´°ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ
- **ç¶™ç¶šçš„å­¦ç¿’**: éå»ã®åˆ†æçµæœã®æ´»ç”¨

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æŠ€è¡“çš„åŠ¹æœ

### ç²¾åº¦å‘ä¸ŠæŒ‡æ¨™
- **å‹•ä½œèªè­˜ç²¾åº¦**: 95%ä»¥ä¸Šï¼ˆå¾“æ¥85%ï¼‰
- **æ–‡æ›¸ç†è§£ç²¾åº¦**: 98%ä»¥ä¸Šï¼ˆOCR + æ„å‘³ç†è§£ï¼‰
- **ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å“è³ª**: å°‚é–€ç”¨èªä½¿ç”¨ç‡90%ä»¥ä¸Š
- **ä¸€è²«æ€§ã‚¹ã‚³ã‚¢**: 95%ä»¥ä¸Šï¼ˆè¤‡æ•°ç”Ÿæˆã§ã®ä¸€è²«æ€§ï¼‰

### å‡¦ç†åŠ¹ç‡åŒ–
- **çµ±åˆå‡¦ç†**: å¾“æ¥ã®6ã¤ã®AIãƒ„ãƒ¼ãƒ« â†’ Gemini 1ã¤ã«é›†ç´„
- **é–‹ç™ºå·¥æ•°**: 60%å‰Šæ¸›ï¼ˆAPIçµ±åˆã®ç°¡ç´ åŒ–ï¼‰
- **é‹ç”¨ã‚³ã‚¹ãƒˆ**: 40%å‰Šæ¸›ï¼ˆå˜ä¸€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ´»ç”¨ï¼‰

ã“ã®ä»•æ§˜ã«ã‚ˆã‚Šã€Gemini 2.5 Proã®å…ˆé€²çš„ãªæ©Ÿèƒ½ã‚’æœ€å¤§é™æ´»ç”¨ã—ãŸã€è£½é€ æ¥­ç‰¹åŒ–ã®é«˜ç²¾åº¦ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚
