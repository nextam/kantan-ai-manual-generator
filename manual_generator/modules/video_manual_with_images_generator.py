"""å‹•ç”»è§£æãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆç”»åƒã‚ã‚Šï¼‰

ãƒãƒªã‚·ãƒ¼: vertexai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä½¿ç”¨ã›ãšã€google-genai ã® Vertex ãƒ¢ãƒ¼ãƒ‰ (client = genai.Client(vertexai=True,...)) ã‚’å¸¸ã«åˆ©ç”¨ã€‚
"""
from __future__ import annotations

import os, cv2, json, base64, numpy as np, logging
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
from google import genai  # type: ignore
from google.genai import types  # type: ignore
Part = types.Part  # google-genai Part ã‚’ç›´æ¥ä½¿ç”¨

logger = logging.getLogger(__name__)

# GCSå‹•ç”»ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥: åŒä¸€ gs:// å‹•ç”»ã®å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é¿ã‘ã‚‹ (ãƒ—ãƒ­ã‚»ã‚¹å­˜ç¶šä¸­ã®ã¿æœ‰åŠ¹)
_GCS_VIDEO_LOCAL_CACHE: Dict[str, str] = {}


class ManualWithImagesGenerator:
    """ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  (google-genai / vertexãƒ¢ãƒ¼ãƒ‰å›ºå®š)"""

    def __init__(self, project_id: str | None = None, location: str = "us-central1") -> None:
        # .env èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
        try:
            from dotenv import load_dotenv
            load_dotenv()
        except Exception:
            pass
        self.project_id = project_id or os.getenv('GOOGLE_CLOUD_PROJECT_ID')
        self.location = location
        if not self.project_id:
            raise ValueError("GOOGLE_CLOUD_PROJECT_ID ãŒæœªè¨­å®šã§ã™")
        creds = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        if creds and not os.path.isabs(creds):
            base = Path(__file__).resolve().parents[1]
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str((base / creds).resolve())
        self._model_name = "gemini-2.5-pro"
        self._client = genai.Client(vertexai=True, project=self.project_id, location=self.location)  # type: ignore
        logger.info("google-genai Vertexãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–å®Œäº† (manual with images)")

    def _generate_content(self, parts):
        return self._client.models.generate_content(model=self._model_name, contents=parts)  # type: ignore

    # ---------- å…±é€š: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º ----------
    def extract_video_samples(self, video_path: str, sample_count: int = 15) -> List[Dict[str, Any]]:
        temp_local_path = None
        original_path = video_path
        # gs:// å¯¾å¿œ: ä¸€æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ OpenCV ã«æ¸¡ã™
        if video_path.startswith('gs://'):
            try:
                import tempfile
                from google.cloud import storage  # Lazy import for environments without GCS
                bucket_name_path = video_path[5:]
                bucket_name, blob_path = bucket_name_path.split('/', 1)
                client = storage.Client()
                bucket = client.bucket(bucket_name)
                blob = bucket.blob(blob_path)
                suffix = os.path.splitext(blob_path)[1] or '.mp4'
                fd, temp_local_path = tempfile.mkstemp(suffix=suffix)
                os.close(fd)
                blob.download_to_filename(temp_local_path)
                logger.info(f"GCSå‹•ç”»ã‚’ä¸€æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {video_path} -> {temp_local_path}")
                video_path = temp_local_path
            except Exception as e:
                raise RuntimeError(f"GCSå‹•ç”»ã®ä¸€æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {video_path} ({e})")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            # å¤±æ•—æ™‚ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if temp_local_path and os.path.exists(temp_local_path):
                try: os.remove(temp_local_path)
                except Exception: pass
            raise ValueError(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {original_path}")

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
        frame_interval = max(1, total_frames // max(1, sample_count))

        frames: List[Dict[str, Any]] = []
        current = 0
        while current < total_frames and len(frames) < sample_count:
            cap.set(cv2.CAP_PROP_POS_FRAMES, current)
            ok, frame = cap.read()
            if not ok:
                break
            ts = (current / fps) if fps else 0.0
            frames.append({
                "frame_number": current,
                "timestamp": ts,
                "timestamp_formatted": f"{int(ts//60):02d}:{int(ts%60):02d}",
                "frame": frame,
            })
            current += frame_interval

        cap.release()
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if temp_local_path and os.path.exists(temp_local_path):
            try:
                os.remove(temp_local_path)
                logger.debug(f"ä¸€æ™‚GCSå‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_local_path}")
            except Exception:
                pass
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º: {len(frames)}ä»¶")
        return frames

    # ---------- Stage 1: ä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—åˆ†æ ----------
    def stage_1_analyze_work_steps(self, video_path: str, custom_prompt: Dict[str, Any] | None = None) -> Dict[str, Any]:
        logger.info("=== 1æ®µéš: ä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—åˆ†æé–‹å§‹ ===")
        frames = self.extract_video_samples(video_path, sample_count=15)

        # ç”»åƒãƒ‘ãƒ¼ãƒ„
        image_parts = []
        for fd in frames:
            # JPEGã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            ok, buf = cv2.imencode('.jpg', fd['frame'], [cv2.IMWRITE_JPEG_QUALITY, 80])
            if not ok:
                continue
            jpeg_bytes = buf.tobytes()
            # å¤ã„ google-genai 1.1.0 ã«ã¯ Part.from_data ãŒç„¡ã„ãŸã‚ from_bytes ã‚’ä½¿ç”¨
            image_parts.append(Part.from_bytes(data=jpeg_bytes, mime_type="image/jpeg"))

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æƒ…å ±ã®å–å¾—
        user_title = (custom_prompt or {}).get('title') or ''
        user_description = (custom_prompt or {}).get('description') or ''
        purpose = (custom_prompt or {}).get('purpose') or user_description or ''
        extra = (custom_prompt or {}).get('custom_instruction') or ''
        
        # ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
        title_info = f"ã€å¯¾è±¡ä½œæ¥­ã€‘{user_title}\n" if user_title else ""
        
        if extra:
            style_directives = (
                f"{title_info}"
                "ã€åŸ·ç­†ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡ç¤ºã€‘\n"
                f"{extra}\n\n"
                f"- ç›®çš„: {purpose or 'ç¾å ´ã§æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«'}\n"
                "- åŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ«: ä½“è¨€æ­¢ã‚ã®ç®‡æ¡æ›¸ãï¼ˆåè©/åè©å¥ã§çµ‚ãˆã‚‹ï¼‰ã€‚å‹•è©çµ‚æ­¢å½¢ã¯é¿ã‘ã‚‹ã€‚\n"
                "- ç®‡æ¡æ›¸ãä¾‹: ã€å›³é¢ç¢ºèªã€ã€éƒ¨ææ•°é‡ãƒã‚§ãƒƒã‚¯ã€ã€ä»®ç· ã‚å®Ÿæ–½ã€ãªã©ã€‚\n"
            )
        else:
            style_directives = (
                f"{title_info}"
                "ã€åŸ·ç­†ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡ç¤ºã€‘\n"
                f"- ç›®çš„: {purpose or 'ç¾å ´ã§æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«'}\n"
                "- åŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ«: ä½“è¨€æ­¢ã‚ã®ç®‡æ¡æ›¸ãï¼ˆåè©/åè©å¥ã§çµ‚ãˆã‚‹ï¼‰ã€‚å‹•è©çµ‚æ­¢å½¢ã¯é¿ã‘ã‚‹ã€‚\n"
                "- å¿…è¦ã«å¿œã˜ã¦å®‰å…¨ä¸Šã®æ³¨æ„ã‚’æ˜è¨˜ã™ã‚‹\n"
                "- ç®‡æ¡æ›¸ãä¾‹: ã€å›³é¢ç¢ºèªã€ã€éƒ¨ææ•°é‡ãƒã‚§ãƒƒã‚¯ã€ã€ä»®ç· ã‚å®Ÿæ–½ã€ãªã©ã€‚\n"
            )

        frames_list_text = [f"ãƒ•ãƒ¬ãƒ¼ãƒ {i+1}: {f['timestamp_formatted']}" for i, f in enumerate(frames)]
        prompt = (
            "ã‚ãªãŸã¯è£½é€ æ¥­ã®ä½œæ¥­æ‰‹é †æ›¸ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸå‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‚’åˆ†æã—ã€ä½œæ¥­ã®æµã‚Œã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚\n\n"
            f"å‹•ç”»æƒ…å ±:\n- ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(frames)}\n- å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {frames_list_text}\n\n"
            f"{style_directives}\n\n"
            "é‡è¦: å¿…ãšä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãšã€JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
            "{\n"
            "  \"work_title\": \"ä½œæ¥­å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«\",\n"
            "  \"work_type\": \"ä½œæ¥­ã®ç¨®é¡ï¼ˆçµ„ç«‹ã€æ¤œæŸ»ã€ãªã©ï¼‰\",\n"
            "  \"estimated_duration\": \"äºˆæƒ³ä½œæ¥­æ™‚é–“ï¼ˆåˆ†ï¼‰\",\n"
            "  \"difficulty_level\": \"åˆç´š/ä¸­ç´š/ä¸Šç´š\",\n"
            "  \"work_steps\": [\n"
            "    {\n"
            "      \"step_number\": 1,\n"
            "      \"step_title\": \"ä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¤ãƒˆãƒ«\",\n"
            "      \"step_description\": \"ä½œæ¥­å†…å®¹ã®è©³ç´°èª¬æ˜ï¼ˆæ–‡ä½“ã¨æ–‡é‡æŒ‡ç¤ºã«åˆã‚ã›ã‚‹ï¼‰\",\n"
            "      \"start_timestamp\": \"00:15\",\n"
            "      \"end_timestamp\": \"00:45\",\n"
            "      \"start_seconds\": 15.0,\n"
            "      \"end_seconds\": 45.0,\n"
            "      \"representative_frame\": 3,\n"
            "      \"key_actions\": [\"å…·ä½“çš„ãªå‹•ä½œ1\", \"å…·ä½“çš„ãªå‹•ä½œ2\"],\n"
            "      \"important_points\": [\"é‡è¦ãƒã‚¤ãƒ³ãƒˆ1\", \"é‡è¦ãƒã‚¤ãƒ³ãƒˆ2\"],\n"
            "      \"safety_notes\": \"å®‰å…¨ä¸Šã®æ³¨æ„ç‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\"\n"
            "    }\n"
            "  ],\n"
            "  \"required_tools\": [\"å¿…è¦ãªå·¥å…·ãƒªã‚¹ãƒˆ\"],\n"
            "  \"materials\": [\"ä½¿ç”¨ã™ã‚‹ææ–™\"],\n"
            "  \"overall_notes\": \"å…¨ä½“çš„ãªæ³¨æ„äº‹é …\"\n"
            "}\n\n"
            "æŒ‡ç¤º:\n"
            "1. ç”»åƒã‚’æ™‚ç³»åˆ—ã§åˆ†æã—ã€æ˜ç¢ºãªä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®š\n"
            "2. å„ã‚¹ãƒ†ãƒƒãƒ—ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ¨å®š\n"
            "3. ä»£è¡¨çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’æŒ‡å®šï¼ˆ1ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã®ç¯„å›²ï¼‰\n"
            "4. ç¾å ´ã§ä½¿ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªå†…å®¹ã«ã™ã‚‹\n"
            "5. æ—¥æœ¬èªã§å›ç­”\n"
            "6. JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å³å¯†ã«å®ˆã‚Šã€ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„\n"
        )

        try:
            contents = [prompt] + image_parts
            response = self._generate_content(contents)
            text = (response.text or '').strip()
            if text.startswith('```json'):
                text = text[7:]
            if text.startswith('```'):
                text = text[3:]
            if text.endswith('```'):
                text = text[:-3]
            text = text.strip()
            data = json.loads(text)
            data.update({
                "stage": 1,
                "timestamp": datetime.now().isoformat(),
                "video_path": video_path,
                "sample_frames_count": len(frames),
            })
            logger.info(f"1æ®µéšå®Œäº†: {len(data.get('work_steps', []))}ã‚¹ãƒ†ãƒƒãƒ—åˆ†æ")
            return data
        except json.JSONDecodeError as e:
            logger.error("1æ®µéš: JSONè§£æå¤±æ•—")
            return {
                "stage": 1,
                "error": f"JSON decode error: {e}",
                "raw_response": text[:1000] if 'text' in locals() else '',
                "work_steps": [],
                "timestamp": datetime.now().isoformat(),
            }
        except Exception as e:
            raise RuntimeError(f"1æ®µéšã®ä½œæ¥­åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # ---------- Stage 1 (hybrid/text-only) ----------
    def stage_1_analyze_work_steps_text_only(self, video_path: str, custom_prompt: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºãªã—ã§å‹•ç”» URI ã‚’ç›´æ¥è§£æã—æ‰‹é † JSON ã‚’å¾—ã‚‹è»½é‡ç‰ˆ"""
        logger.info("=== 1æ®µéš(hybrid): ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è§£æé–‹å§‹ ===")
        if not video_path.startswith('gs://'):
            logger.warning("hybrid stage1 ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ãŒæ¸¡ã•ã‚Œã¾ã—ãŸã€‚æ¨å¥¨ã¯ GCS URI")
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æƒ…å ±ã®å–å¾—
        user_title = (custom_prompt or {}).get('title') or ''
        user_description = (custom_prompt or {}).get('description') or ''
        purpose = (custom_prompt or {}).get('purpose') or user_description or ''
        extra = (custom_prompt or {}).get('custom_instruction') or ''
        
        # ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
        title_instruction = f"å¯¾è±¡ä½œæ¥­: {user_title}\n" if user_title else ""
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€å„ªå…ˆã§é…ç½®
        if extra:
            prompt = f"""ã‚ãªãŸã¯è£½é€ æ¥­ã®ä½œæ¥­æ‰‹é †æ›¸ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚

{title_instruction}{extra}

å‹•ç”»å…¨ä½“ã‚’åˆ†æã—ä½œæ¥­æ‰‹é †ã‚’ JSON å½¢å¼ã§è¿”å´ã—ã¦ãã ã•ã„ã€‚
ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å¾Œæ®µã§ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ step ã®ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã¯é€£ç•ªä»®å€¤ã§æ§‹ã„ã¾ã›ã‚“ã€‚
ç›®çš„: {purpose or 'ç¾å ´ã§æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªãƒãƒ‹ãƒ¥ã‚¢ãƒ«'}
å‡ºåŠ›ã¯æ—¢å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ JSON ã®ã¿ã€‚ä»–ã®æ–‡å­—åˆ—ç¦æ­¢ã€‚"""
        else:
            prompt = (
                f"ã‚ãªãŸã¯è£½é€ æ¥­ã®ä½œæ¥­æ‰‹é †æ›¸ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚{title_instruction}å‹•ç”»å…¨ä½“ã‚’åˆ†æã—ä½œæ¥­æ‰‹é †ã‚’ JSON å½¢å¼ã§è¿”å´ã—ã¦ãã ã•ã„ã€‚"
                "ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å¾Œæ®µã§ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ step ã®ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã¯é€£ç•ªä»®å€¤ã§æ§‹ã„ã¾ã›ã‚“ã€‚"
                f"ç›®çš„: {purpose or 'ç¾å ´ã§æ´»ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªãƒãƒ‹ãƒ¥ã‚¢ãƒ«'}\n"
                "å®‰å…¨/å“è³ªä¸Šã®ç•™æ„ç‚¹ã‚’å«ã‚ã¦ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
                "å‡ºåŠ›ã¯æ—¢å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ JSON ã®ã¿ã€‚ä»–ã®æ–‡å­—åˆ—ç¦æ­¢ã€‚"
            )
        part = Part.from_uri(file_uri=video_path, mime_type='video/mp4')
        try:
            resp = self._generate_content([prompt, part])
            text = (resp.text or '').strip()
            if text.startswith('```json'):
                text = text[7:]
            if text.startswith('```'):
                text = text[3:]
            if text.endswith('```'):
                text = text[:-3]
            data = json.loads(text)
            steps = data.get('work_steps', [])
            for i, s in enumerate(steps, 1):
                s.setdefault('representative_frame', i)
            data.update({'stage':1,'mode':'hybrid_text_only','sample_frames_count':0,'video_path':video_path})
            logger.info(f"1æ®µéš(hybrid) å®Œäº†: {len(steps)}ã‚¹ãƒ†ãƒƒãƒ—")
            return data
        except Exception as e:
            raise RuntimeError(f"1æ®µéš(hybrid) è§£æå¤±æ•—: {e}")

    # ---------- Stage 2: ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º ----------
    def fix_frame_orientation(self, frame: np.ndarray, video_path: str) -> np.ndarray:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä¸Šä¸‹ã‚’å¼·åˆ¶è£œæ­£ (å¸¸ã«ç¸¦æ–¹å‘åè»¢)"""
        try:
            from utils.frame_orientation import enforce_vertical_orientation, ALWAYS_FLIP_VERTICAL, ALWAYS_FLIP_HORIZONTAL
            flipped = enforce_vertical_orientation(frame)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"generator.orientation.flip vertical={ALWAYS_FLIP_VERTICAL} horizontal={ALWAYS_FLIP_HORIZONTAL} size={frame.shape if frame is not None else None}")
            return flipped
        except Exception:
            # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”å´
            return frame

    def stage_2_extract_representative_frames(self, video_path: str, stage1_result: Dict[str, Any]) -> Dict[str, Any]:
        logger.info("=== 2æ®µéš: ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹ (ã‚¼ãƒ­ãƒ¢ãƒ‡ãƒ«å‰å‡¦ç† + Geminiå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°) ===")
        if not stage1_result.get('work_steps'):
            raise ValueError("1æ®µéšã®çµæœã«ä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        def _laplacian_sharpness(img: np.ndarray) -> float:
            try:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                return float(cv2.Laplacian(gray, cv2.CV_64F).var())
            except Exception:
                return 0.0

        def _brightness(img: np.ndarray) -> float:
            try:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                return float(np.mean(gray))
            except Exception:
                return 0.0

        def _normalize(values: List[float]) -> List[float]:
            if not values:
                return values
            vmin, vmax = min(values), max(values)
            if vmax - vmin < 1e-6:
                return [0.5 for _ in values]
            return [(v - vmin) / (vmax - vmin) for v in values]

        def _candidate_timestamps(start_s: float, end_s: float) -> List[float]:
            if end_s <= start_s:
                end_s = start_s + 5.0
            dur = end_s - start_s
            return sorted({
                round(start_s + dur * 0.25, 3),
                round(start_s + dur * 0.5, 3),
                round(start_s + dur * 0.75, 3)
            })

        def _gemini_rank(step_meta: Dict[str, Any], candidates: List[Dict[str, Any]]) -> int:
            if not candidates:
                return -1
            # Vertex AI åˆ©ç”¨ä¸å¯ã®å ´åˆã¯ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹æœ€å¤§
            # rerank ã¯å¸¸ã« gemini åˆ©ç”¨ (fallback ãªã—) æƒ³å®š
            try:
                prompt = (
                    "ã‚ãªãŸã¯è£½é€ æ¥­ã®ä½œæ¥­æ‰‹é †å¯è¦–åŒ–ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å€™è£œç”»åƒã‹ã‚‰ãã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æœ€ã‚‚æ˜ç¢ºã«ç¤ºã™1æšã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
                    "å‡ºåŠ›ã¯JSONã®ã¿ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {\n"
                    "  \"selected_index\": <0-based index>,\n"
                    "  \"confidence\": 0.0ã€œ1.0,\n"
                    "  \"scores\": [ { \"index\": i, \"relevance\":1-5, \"clarity\":1-5, \"stability\":1-5 } ]\n"
                    "}\n"
                    f"ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·: {step_meta.get('step_number')} ã‚¿ã‚¤ãƒˆãƒ«: {step_meta.get('step_title')}\n"
                    f"èª¬æ˜: {step_meta.get('step_description','')}\n"
                    "è©•ä¾¡è¦³ç‚¹: relevance=å‹•ä½œã®æ ¸å¿ƒãŒå†™ã£ã¦ã„ã‚‹ã‹ / clarity=ãƒ”ãƒ³ã¼ã‘ã‚„æš—ã•ãŒå°‘ãªã„ / stability=ãƒ–ãƒ¬ã‚„é€”ä¸­é·ç§»ã®ç¬é–“ã§ãªã„ã€‚"
                )
                parts = [prompt]
                for c in candidates:
                    parts.append(Part.from_bytes(data=base64.b64decode(c['image_base64']), mime_type='image/jpeg'))
                response = self._generate_content(parts)
                txt = (response.text or '').strip()
                if txt.startswith('```json'):
                    txt = txt[7:]
                if txt.startswith('```'):
                    txt = txt[3:]
                if txt.endswith('```'):
                    txt = txt[:-3]
                txt = txt.strip()
                data = json.loads(txt)
                idx = int(data.get('selected_index', 0))
                if 0 <= idx < len(candidates):
                    return idx
                return int(np.argmax([c['sharpness'] for c in candidates]))
            except Exception as e:
                logger.warning(f"Geminiå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤±æ•—: {e}. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹fallback")
                return int(np.argmax([c['sharpness'] for c in candidates]))

        # ---- gs:// ãƒ‘ã‚¹ã‚’ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿä½“ã¸ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ----
        local_path = video_path
        if video_path.startswith('gs://'):
            cached = _GCS_VIDEO_LOCAL_CACHE.get(video_path)
            if cached and os.path.exists(cached):
                local_path = cached
                logger.debug(f"GCSå‹•ç”»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨(Stage2): {video_path} -> {local_path}")
            else:
                try:
                    import tempfile
                    from google.cloud import storage  # type: ignore
                    bucket_name, blob_path = video_path[5:].split('/', 1)
                    client = storage.Client(); bucket = client.bucket(bucket_name); blob = bucket.blob(blob_path)
                    fd, tmp_path = tempfile.mkstemp(suffix=os.path.splitext(blob_path)[1] or '.mp4'); os.close(fd)
                    blob.download_to_filename(tmp_path)
                    _GCS_VIDEO_LOCAL_CACHE[video_path] = tmp_path
                    local_path = tmp_path
                    logger.info(f"GCSå‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰(Stage2): {video_path} -> {tmp_path}")
                except Exception as e:
                    raise RuntimeError(f"Stage2: GCSå‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— {video_path}: {e}")

        cap = cv2.VideoCapture(local_path)
        if not cap.isOpened():
            raise ValueError(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")
        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0

        extracted: List[Dict[str, Any]] = []
        ranking_metadata: List[Dict[str, Any]] = []

        for step in stage1_result['work_steps']:
            step_number = step.get('step_number', 0)
            start_seconds = float(step.get('start_seconds', 0.0))
            end_seconds = float(step.get('end_seconds', start_seconds + 5.0))
            ts_list = _candidate_timestamps(start_seconds, end_seconds)

            raw_candidates: List[Dict[str, Any]] = []
            prev_frame_small = None
            for ts in ts_list:
                frame_index = int(ts * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                ok, frame = cap.read()
                if not ok:
                    continue
                frame = self.fix_frame_orientation(frame, video_path)
                sharp = _laplacian_sharpness(frame)
                bright = _brightness(frame)
                # ç°¡æ˜“ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³: å‰å€™è£œã¨ã®MSE
                motion_pen = 0.0
                try:
                    small = cv2.resize(frame, (64, 64))
                    if prev_frame_small is not None:
                        diff = cv2.absdiff(prev_frame_small, small)
                        motion_pen = float(np.mean(diff))
                    prev_frame_small = small
                except Exception:
                    pass
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w = rgb.shape[:2]
                _, buf = cv2.imencode('.jpg', cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, 85])
                b64 = base64.b64encode(buf).decode('utf-8')
                raw_candidates.append({
                    'timestamp_seconds': ts,
                    'timestamp_formatted': f"{int(ts//60):02d}:{int(ts%60):02d}",
                    'frame_number': frame_index,
                    'image_base64': b64,
                    'width': w,
                    'height': h,
                    'sharpness': sharp,
                    'brightness': bright,
                    'motion_penalty': motion_pen,
                })

            if not raw_candidates:
                continue
            # æ­£è¦åŒ– & ã‚¹ã‚³ã‚¢è¨ˆç®—
            sharp_norm = _normalize([c['sharpness'] for c in raw_candidates])
            bright_vals = [c['brightness'] for c in raw_candidates]
            bright_norm = _normalize(bright_vals)
            # ç›®æ¨™æ˜åº¦ 0.5 ã‹ã‚‰ã®è·é›¢ã‚’æ¸›ç‚¹
            scored_candidates = []
            for idx, c in enumerate(raw_candidates):
                score = (
                    0.6 * sharp_norm[idx]
                    + 0.3 * (1 - abs(bright_norm[idx] - 0.5) * 2)  # ä¸­é–“æ˜ã‚‹ã•å„ªé‡
                    + 0.1 * (1 - min(1.0, c['motion_penalty'] / 50.0))
                )
                sc = dict(c)
                sc['heuristic_score'] = score
                scored_candidates.append(sc)

            # Heuristic ä¸Šä½ 3 ã¾ã§
            scored_candidates.sort(key=lambda x: x['heuristic_score'], reverse=True)
            top_candidates = scored_candidates[:3]

            # Gemini å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            sel_index = _gemini_rank({
                'step_number': step_number,
                'step_title': step.get('step_title'),
                'step_description': step.get('step_description','')
            }, top_candidates)
            if sel_index < 0 or sel_index >= len(top_candidates):
                sel_index = 0
            chosen = top_candidates[sel_index]

            extracted.append({
                'step_number': step_number,
                'timestamp_seconds': chosen['timestamp_seconds'],
                'timestamp_formatted': chosen['timestamp_formatted'],
                'frame_number': chosen['frame_number'],
                'image_base64': chosen['image_base64'],
                'image_data_url': f"data:image/jpeg;base64,{chosen['image_base64']}",
                'step_title': step.get('step_title', f'ã‚¹ãƒ†ãƒƒãƒ— {step_number}'),
                'step_description': step.get('step_description', ''),
                'width': chosen['width'],
                'height': chosen['height'],
                'selection_method': 'gemini_rerank',
                # (2) ä»¥é™ã§åˆ©ç”¨: åˆæœŸå›è»¢è§’ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç·¨é›†ç”¨ / 0,90,180,270 ã®ã¿æƒ³å®šï¼‰
                'rotation': 0,
            })

            ranking_metadata.append({
                'step_number': step_number,
                'candidates': [
                    {
                        'timestamp': c['timestamp_formatted'],
                        'sharpness': c['sharpness'],
                        'brightness': c['brightness'],
                        'motion_penalty': c['motion_penalty'],
                        'heuristic_score': c['heuristic_score']
                    } for c in top_candidates
                ],
                'selected_index': sel_index
            })

        cap.release()
        result = {
            'stage': 2,
            'timestamp': datetime.now().isoformat(),
            'video_path': video_path,
            'extracted_frames': extracted,
            'total_frames': len(extracted),
            'ranking_metadata': ranking_metadata,
            'stage1_reference': {
                'work_title': stage1_result.get('work_title', ''),
                'work_steps_count': len(stage1_result.get('work_steps', [])),
            },
        }
        logger.info(f"2æ®µéšå®Œäº†: {len(extracted)}ãƒ•ãƒ¬ãƒ¼ãƒ ç¢ºå®š (å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°é©ç”¨)")
        return result

    # ---------- Stage 2 (hybrid minimal extraction) ----------
    def stage_2_extract_representative_frames_hybrid(self, video_path: str, stage1_result: Dict[str, Any]) -> Dict[str, Any]:
        """å„ã‚¹ãƒ†ãƒƒãƒ— midpoint ã®1æšã®ã¿æŠ½å‡ºã™ã‚‹è»½é‡ç‰ˆ (Stage3 äº’æ›å½¢å¼)"""
        logger.info("=== 2æ®µéš(hybrid): æœ€å°ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹ ===")
        steps = stage1_result.get('work_steps') or []
        if not steps:
            raise ValueError('hybrid Stage2: work_steps ãŒç©º')
        local_path = video_path
        if video_path.startswith('gs://'):
            cached = _GCS_VIDEO_LOCAL_CACHE.get(video_path)
            if cached and os.path.exists(cached):
                local_path = cached
                logger.debug(f"GCSå‹•ç”»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨(hybrid Stage2): {video_path} -> {local_path}")
            else:
                try:
                    import tempfile
                    from google.cloud import storage  # type: ignore
                    bucket_name, blob_path = video_path[5:].split('/',1)
                    client = storage.Client(); bucket = client.bucket(bucket_name); blob = bucket.blob(blob_path)
                    fd, tmp_path = tempfile.mkstemp(suffix=os.path.splitext(blob_path)[1] or '.mp4'); os.close(fd)
                    blob.download_to_filename(tmp_path)
                    _GCS_VIDEO_LOCAL_CACHE[video_path] = tmp_path
                    local_path = tmp_path
                    logger.info(f"GCSå‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰(hybrid Stage2): {video_path} -> {tmp_path}")
                except Exception as e:
                    raise RuntimeError(f"hybrid Stage2: GCSå‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— {video_path}: {e}")
        cap = cv2.VideoCapture(local_path)
        if not cap.isOpened():
            raise RuntimeError('hybrid Stage2: å‹•ç”»ã‚ªãƒ¼ãƒ—ãƒ³å¤±æ•—')
        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
        extracted: List[Dict[str, Any]] = []
        for step in steps:
            ss = float(step.get('start_seconds',0.0))
            es = float(step.get('end_seconds', ss+2.0))
            mid = ss + (es-ss)*0.5
            frame_index = int(mid*fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ok, frame = cap.read()
            if not ok:
                continue
            frame = self.fix_frame_orientation(frame, video_path)
            h, w = frame.shape[:2]
            _, buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY,80])
            b64 = base64.b64encode(buf).decode('utf-8')
            extracted.append({
                'step_number': step.get('step_number'),
                'timestamp_seconds': mid,
                'timestamp_formatted': f"{int(mid//60):02d}:{int(mid%60):02d}",
                'frame_number': frame_index,
                'image_base64': b64,
                'image_data_url': f"data:image/jpeg;base64,{b64}",
                'step_title': step.get('step_title'),
                'step_description': step.get('step_description',''),
                'width': w,
                'height': h,
                'selection_method': 'hybrid_midpoint',
                'rotation': 0,
            })
        cap.release()
    # cleanup ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–æˆ¦ç•¥ã®ãŸã‚è¡Œã‚ãªã„
        result = {
            'stage':2,
            'mode':'hybrid_minimal',
            'video_path': video_path,
            'extracted_frames': extracted,
            'total_frames': len(extracted),
            'ranking_metadata': [],
            'stage1_reference': {
                'work_title': stage1_result.get('work_title',''),
                'work_steps_count': len(steps)
            }
        }
        logger.info(f"2æ®µéš(hybrid) å®Œäº†: {len(extracted)}æš")
        return result

    # ---------- Hybrid pipeline convenience ----------
    def run_hybrid_pipeline(self, video_path: str, custom_prompt: Dict[str, Any] | None = None) -> Dict[str, Any]:
        s1 = self.stage_1_analyze_work_steps_text_only(video_path, custom_prompt)
        s2 = self.stage_2_extract_representative_frames_hybrid(video_path, s1)
        return {'stage1': s1, 'stage2': s2}

    # ---------- Stage 3: HTML ç”Ÿæˆï¼ˆäºŒåˆ†å‰²ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰ ----------
    def stage_3_generate_html_manual(self, stage1_result: Dict[str, Any], stage2_result: Dict[str, Any], custom_prompt: Dict[str, Any] | None = None) -> str:
        logger.info("=== ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆ: HTMLãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆé–‹å§‹ ===")
        work_title = stage1_result.get('work_title', 'å‹•ç”»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«')
        work_steps = stage1_result.get('work_steps', [])
        frames_by_step = {f['step_number']: f for f in stage2_result.get('extracted_frames', [])}
        output_detail = (custom_prompt or {}).get('output_detail', 'titles_only')

        html = []
        html.append("<!DOCTYPE html>\n<html lang=\"ja\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>")
        html.append(self._escape(work_title))
        html.append(" - ä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«</title>\n</head>\n<body style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Hiragino Sans', Meiryo, sans-serif; line-height:1.6; margin:0; padding:16px; background:#ffffff; color:#222;\">\n  <div style=\"max-width:1200px; margin:0 auto;\">\n    <div style=\"padding:12px 0 20px;\">\n      <h1 style=\"margin:0 0 8px; font-size:22px; color:#0d47a1;\">")
        html.append(self._escape(work_title))
        html.append("</h1>\n      <div style=\"font-size:13px; color:#555;\">ç¨®é¡: ")
        html.append(self._escape(stage1_result.get('work_type', 'ä¸€èˆ¬ä½œæ¥­')))
        html.append(" ï¼ é›£æ˜“åº¦: ")
        html.append(self._escape(stage1_result.get('difficulty_level', 'ä¸­ç´š')))
        html.append(" ï¼ äºˆæƒ³æ™‚é–“: ")
        html.append(self._escape(str(stage1_result.get('estimated_duration', '30'))))
        html.append("åˆ† ï¼ ã‚¹ãƒ†ãƒƒãƒ—æ•°: ")
        html.append(str(len(work_steps)))
        # å…ˆã« å·¥å…·ãƒ»ææ–™ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é…ç½®
        html.append("</div>\n    </div>\n")
        if stage1_result.get('required_tools') or stage1_result.get('materials'):
            html.append("    <div style=\"margin:0 0 16px; display:flex; gap:12px; flex-wrap:wrap;\">\n")
            if stage1_result.get('required_tools'):
                html.append("      <div style=\"flex:1 1 360px; min-width:280px; border:1px solid #e5e7eb; border-radius:8px; padding:12px;\">\n        <div style=\"font-weight:600; color:#222; margin-bottom:6px;\">ğŸ”§ å¿…è¦ãªå·¥å…·</div>\n        <ul style=\"margin:0; padding-left:18px;\">\n")
                for tool in stage1_result.get('required_tools', []):
                    html.append("          <li>")
                    html.append(self._escape(tool))
                    html.append("</li>\n")
                html.append("        </ul>\n      </div>\n")
            if stage1_result.get('materials'):
                html.append("      <div style=\"flex:1 1 360px; min-width:280px; border:1px solid #e5e7eb; border-radius:8px; padding:12px;\">\n        <div style=\"font-weight:600; color:#222; margin-bottom:6px;\">ğŸ“¦ ä½¿ç”¨ææ–™</div>\n        <ul style=\"margin:0; padding-left:18px;\">\n")
                for material in stage1_result.get('materials', []):
                    html.append("          <li>")
                    html.append(self._escape(material))
                    html.append("</li>\n")
                html.append("        </ul>\n      </div>\n")
            html.append("    </div>\n")
        # ä½œæ¥­æ‰‹é †+ç”»åƒã®äºŒåˆ†å‰²ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        html.append("    <div style=\"display:flex; gap:16px; align-items:flex-start; flex-wrap:wrap;\">\n      <div style=\"flex:1 1 200px; min-width:200px; border:1px solid #e5e7eb; border-radius:8px; padding:16px; box-shadow:0 1px 3px rgba(0,0,0,0.05);\">\n        <h2 style=\"margin:0 0 12px; font-size:18px; color:#222; border-left:4px solid #1565c0; padding-left:10px;\">ä½œæ¥­æ‰‹é †</h2>\n        <ol style=\"margin:0; padding-left:20px;\">\n")

        for step in work_steps:
            n = step.get('step_number', 1)
            title = step.get('step_title', f'ã‚¹ãƒ†ãƒƒãƒ— {n}')
            desc = step.get('step_description', '')
            key_actions = step.get('key_actions', [])
            important = step.get('important_points', [])
            safety = step.get('safety_notes', '')

            html.append("          <li style=\"margin:0 0 10px;\">\n            <div style=\"font-weight:600; color:#222;\">ã‚¹ãƒ†ãƒƒãƒ— ")
            html.append(str(n))
            html.append(": ")
            html.append(self._escape(title))
            html.append("</div>\n")

            if output_detail == 'titles_with_descriptions' and desc:
                html.append("            <div style=\"margin:4px 0 0; color:#555;\">")
                html.append(self._escape(desc))
                html.append("</div>\n")

            if output_detail == 'titles_with_descriptions' and key_actions:
                html.append("            <ul style=\"margin:6px 0 0 16px; padding:0; list-style: disc; color:#333;\">\n")
                for act in key_actions:
                    html.append("              <li style=\"margin:2px 0;\">")
                    html.append(self._escape(act))
                    html.append("</li>\n")
                html.append("            </ul>\n")

            if output_detail == 'titles_with_descriptions' and important:
                html.append("            <ul style=\"margin:6px 0 0 16px; padding:0; list-style: circle; color:#333;\">\n")
                for p in important:
                    html.append("              <li style=\"margin:2px 0;\">")
                    html.append(self._escape(p))
                    html.append("</li>\n")
                html.append("            </ul>\n")

            if output_detail == 'titles_with_descriptions' and safety:
                html.append("            <div style=\"margin-top:6px; padding:8px; background:#fff8e1; border:1px solid #ffe0b2; border-radius:6px;\">âš ï¸ ")
                html.append(self._escape(safety))
                html.append("</div>\n")

            html.append("          </li>\n")

        html.append("        </ol>\n      </div>\n")

        # å³ã‚«ãƒ©ãƒ 
        html.append("      <div style=\"flex:1 1 200px; min-width:200px; display:flex; flex-direction:row; gap:10px; flex-wrap:wrap;\">\n")
        for step in work_steps:
            n = step.get('step_number', 1)
            title = step.get('step_title', f'ã‚¹ãƒ†ãƒƒãƒ— {n}')
            f = frames_by_step.get(n)
            if f:
                caption = f"ã‚¹ãƒ†ãƒƒãƒ— {n}: {title}"
                ts = f.get('timestamp_formatted', '')
                # ç”»åƒã¯æ—¢ã«ç‰©ç†çš„ã«å›è»¢ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€CSSå›è»¢ã¯ä¸è¦
                html.append(f"        <figure data-step=\"{n}\" style=\"margin:0; padding:10px; border:1px solid #e5e7eb; border-radius:8px; background:#fafafa; box-shadow:0 1px 3px rgba(0,0,0,0.04);\">\n          <img data-step=\"{n}\" src=\"")
                html.append(f['image_data_url'])
                html.append("\" alt=\"")
                html.append(self._escape(caption))
                html.append(f"\" style=\"width:100%; height:auto; display:block; border-radius:6px;\">\n          <figcaption style=\"font-size:13px; color:#333; margin-top:6px;\">")
                html.append(self._escape(caption))
                html.append("<span style=\"color:#666; margin-left:8px;\">")
                html.append(self._escape(ts))
                html.append("</span></figcaption>\n        </figure>\n")
            else:
                html.append(f"        <div data-step=\"{n}\" style=\"padding:16px; border:1px dashed #ccc; border-radius:8px; color:#777; text-align:center;\">ã‚¹ãƒ†ãƒƒãƒ— ")
                html.append(str(n))
                html.append(" ã®ç”»åƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“</div>\n")
        html.append("</div>\n</div>\n\n</div>\n</body>\n</html>\n")

        result = ''.join(html)
        logger.info("ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆå®Œäº†: HTMLãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ")
        return result

    # ---------- Utils ----------
    @staticmethod
    def _escape(s: Any) -> str:
        x = str(s)
        return (
            x.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&#39;")
        )

    # ---------- Convenience: ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆä¸€æ‹¬å®Ÿè¡Œï¼ˆåŒæœŸç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘ã‘ï¼‰ ----------
    def generate_manual_with_images(self, video_path: str, custom_prompt: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆã‚’é †æ¬¡å®Ÿè¡Œã—ã€é›†ç´„çµæœã‚’è¿”ã™ã€‚ä¸»ã«åŒæœŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç”¨ã€‚"""
        logger.info("=== ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆå‡¦ç†é–‹å§‹ï¼ˆåŒæœŸï¼‰ ===")
        try:
            stage1 = self.stage_1_analyze_work_steps(video_path, custom_prompt)
            stage2 = self.stage_2_extract_representative_frames(video_path, stage1)
            html = self.stage_3_generate_html_manual(stage1, stage2, custom_prompt)
            return {
                "success": True,
                "timestamp": datetime.now().isoformat(),
                "video_path": video_path,
                "stage1_result": stage1,
                "stage2_result": stage2,
                "html_manual": html,
                "summary": {
                    "work_title": stage1.get('work_title', ''),
                    "total_steps": len(stage1.get('work_steps', [])),
                    "extracted_frames": len(stage2.get('extracted_frames', [])),
                    "html_length": len(html),
                },
            }
        except Exception as e:
            logger.error(f"ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆç”»åƒã‚ã‚Šï¼‰ç”Ÿæˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆåŒæœŸï¼‰: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "video_path": video_path,
            }

