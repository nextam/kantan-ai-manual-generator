"""
Gemini 2.5 Proçµ±åˆã‚µãƒ¼ãƒ“ã‚¹
è£½é€ æ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã®ãŸã‚ã®åŒ…æ‹¬çš„AIæ©Ÿèƒ½ã‚’æä¾›
"""

import google.generativeai as genai
import vertexai
from vertexai.generative_models import GenerativeModel, Part, FinishReason, Tool, FunctionDeclaration
import vertexai.preview.generative_models as generative_models
from typing import List, Dict, Any, Optional, Union
import json
import base64
import time
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv
import logging

# Load environment variables
load_dotenv()

# Import GCP config helper
import sys
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from src.utils.gcp_config import get_gcp_project_id

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GeminiUnifiedService:
    """Gemini 2.5 Proçµ±åˆã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, project_id: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id: Google Cloud Project ID (optional, auto-detected from credentials if not provided)
        """
        # Get configuration from environment variables or credentials file
        if project_id:
            self.project_id = project_id
        else:
            try:
                self.project_id = get_gcp_project_id()
            except ValueError as e:
                raise ValueError(f"Failed to determine GCP project ID: {e}")
        
        location = os.getenv('VERTEX_AI_LOCATION', 'us-central1')
        
        # Vertex AIåˆæœŸåŒ–
        vertexai.init(project=self.project_id, location=location)
        self.model = GenerativeModel('gemini-2.5-pro')
        
        # å®‰å…¨è¨­å®š
        self.safety_settings = {
            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
        
        # Function Callingå®šç¾©
        self.tools = self._setup_tool_definitions()
        
        logger.info("Gemini 2.5 Proçµ±åˆã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    
    def _setup_tool_definitions(self) -> List[Tool]:
        """VertexAIç”¨ãƒ„ãƒ¼ãƒ«å®šç¾©"""
        
        # ä½œæ¥­æ‰‹é †æŠ½å‡ºãƒ„ãƒ¼ãƒ«
        extract_work_steps = FunctionDeclaration(
            name="extract_work_steps",
            description="å‹•ç”»ã‹ã‚‰ä½œæ¥­æ‰‹é †ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã™ã‚‹",
            parameters={
                "type": "object",
                "properties": {
                    "work_title": {
                        "type": "string",
                        "description": "ä½œæ¥­ã®åç§°"
                    },
                    "estimated_time": {
                        "type": "number",
                        "description": "æ¨å®šä½œæ¥­æ™‚é–“ï¼ˆåˆ†ï¼‰"
                    },
                    "skill_level": {
                        "type": "string",
                        "enum": ["beginner", "intermediate", "expert"],
                        "description": "å¿…è¦ãªæŠ€èƒ½ãƒ¬ãƒ™ãƒ«"
                    },
                    "steps": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "step_number": {"type": "integer", "description": "æ‰‹é †ç•ªå·"},
                                "action": {"type": "string", "description": "å®Ÿè¡Œã™ã‚‹å‹•ä½œ"},
                                "tools_used": {
                                    "type": "array", 
                                    "items": {"type": "string"},
                                    "description": "ä½¿ç”¨ã™ã‚‹å·¥å…·"
                                },
                                "duration_seconds": {"type": "number", "description": "æ‰€è¦æ™‚é–“ï¼ˆç§’ï¼‰"},
                                "safety_notes": {"type": "string", "description": "å®‰å…¨ä¸Šã®æ³¨æ„äº‹é …"},
                                "quality_points": {"type": "string", "description": "å“è³ªç®¡ç†ã®ãƒã‚¤ãƒ³ãƒˆ"},
                                "expert_tips": {"type": "string", "description": "ç†Ÿç·´è€…ã®ã‚³ãƒ„"},
                                "common_mistakes": {"type": "string", "description": "ã‚ˆãã‚ã‚‹å¤±æ•—"}
                            },
                            "required": ["step_number", "action"]
                        }
                    }
                },
                "required": ["work_title", "steps"]
            }
        )
        
        # ä½œæ¥­æŠ€è¡“æ¯”è¼ƒãƒ„ãƒ¼ãƒ«
        compare_work_techniques = FunctionDeclaration(
            name="compare_work_techniques",
            description="ç†Ÿç·´è€…ã¨éç†Ÿç·´è€…ã®ä½œæ¥­æŠ€è¡“ã‚’æ¯”è¼ƒåˆ†æã™ã‚‹",
            parameters={
                "type": "object",
                "properties": {
                    "overall_assessment": {
                        "type": "object",
                        "properties": {
                            "efficiency_gap": {"type": "string", "description": "åŠ¹ç‡æ€§ã®å·®"},
                            "safety_gap": {"type": "string", "description": "å®‰å…¨æ€§ã®å·®"},
                            "quality_gap": {"type": "string", "description": "å“è³ªã®å·®"}
                        }
                    },
                    "detailed_differences": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "aspect": {"type": "string", "description": "æ¯”è¼ƒè¦³ç‚¹"},
                                "expert_approach": {"type": "string", "description": "ç†Ÿç·´è€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"},
                                "novice_approach": {"type": "string", "description": "éç†Ÿç·´è€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"},
                                "improvement_suggestion": {"type": "string", "description": "æ”¹å–„ææ¡ˆ"},
                                "impact_level": {
                                    "type": "string", 
                                    "enum": ["high", "medium", "low"],
                                    "description": "å½±éŸ¿åº¦"
                                },
                                "training_priority": {
                                    "type": "integer",
                                    "description": "ç ”ä¿®å„ªå…ˆåº¦ï¼ˆ1-5ï¼‰"
                                }
                            },
                            "required": ["aspect", "expert_approach", "novice_approach", "improvement_suggestion"]
                        }
                    },
                    "recommended_training": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "skill_area": {"type": "string"},
                                "training_method": {"type": "string"},
                                "expected_improvement": {"type": "string"}
                            }
                        }
                    }
                },
                "required": ["overall_assessment", "detailed_differences"]
            }
        )
        
        # ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç‰¹å®šãƒ„ãƒ¼ãƒ«
        identify_key_frames = FunctionDeclaration(
            name="identify_key_frames",
            description="å‹•ç”»ã‹ã‚‰é‡è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ /ç¬é–“ã‚’ç‰¹å®šã™ã‚‹",
            parameters={
                "type": "object",
                "properties": {
                    "key_moments": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "timestamp": {"type": "number", "description": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆç§’ï¼‰"},
                                "description": {"type": "string", "description": "ã“ã®ç¬é–“ã®èª¬æ˜"},
                                "importance": {
                                    "type": "string",
                                    "enum": ["critical", "important", "helpful"],
                                    "description": "é‡è¦åº¦"
                                },
                                "category": {
                                    "type": "string",
                                    "enum": ["safety", "quality", "efficiency", "technique"],
                                    "description": "ã‚«ãƒ†ã‚´ãƒª"
                                },
                                "annotation_text": {"type": "string", "description": "ç”»åƒã«è¿½åŠ ã™ã¹ãæ³¨é‡ˆ"},
                                "manual_section": {"type": "string", "description": "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³"}
                            },
                            "required": ["timestamp", "description", "importance"]
                        }
                    }
                },
                "required": ["key_moments"]
            }
        )
        
        # æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ„ãƒ¼ãƒ«
        extract_document_data = FunctionDeclaration(
            name="extract_document_data",
            description="æ–‡æ›¸ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹",
            parameters={
                "type": "object",
                "properties": {
                    "document_type": {
                        "type": "string",
                        "description": "æ–‡æ›¸ã®ç¨®é¡ï¼ˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€ä»•æ§˜æ›¸ã€å ±å‘Šæ›¸ãªã©ï¼‰"
                    },
                    "title": {
                        "type": "string",
                        "description": "æ–‡æ›¸ã‚¿ã‚¤ãƒˆãƒ«"
                    },
                    "extracted_text": {
                        "type": "string",
                        "description": "OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡"
                    },
                    "key_information": {
                        "type": "object",
                        "properties": {
                            "specifications": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "parameter": {"type": "string", "description": "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å"},
                                        "value": {"type": "string", "description": "å€¤"},
                                        "unit": {"type": "string", "description": "å˜ä½"}
                                    }
                                },
                                "description": "ä»•æ§˜ãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿"
                            },
                            "safety_warnings": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "å®‰å…¨è­¦å‘Šãƒ»æ³¨æ„äº‹é …"
                            },
                            "quality_standards": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "å“è³ªåŸºæº–ãƒ»è¨±å®¹å€¤"
                            },
                            "procedures": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "step": {"type": "integer", "description": "æ‰‹é †ç•ªå·"},
                                        "description": {"type": "string", "description": "æ‰‹é †å†…å®¹"},
                                        "checkpoints": {
                                            "type": "array",
                                            "items": {"type": "string"},
                                            "description": "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ"
                                        }
                                    }
                                },
                                "description": "ä½œæ¥­æ‰‹é †"
                            }
                        }
                    },
                    "technical_terms": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "term": {"type": "string", "description": "å°‚é–€ç”¨èª"},
                                "definition": {"type": "string", "description": "å®šç¾©ãƒ»æ„å‘³"},
                                "context": {"type": "string", "description": "ä½¿ç”¨æ–‡è„ˆ"}
                            }
                        },
                        "description": "å°‚é–€ç”¨èªé›†"
                    }
                },
                "required": ["document_type", "extracted_text", "key_information"]
            }
        )
        
        return [Tool(function_declarations=[extract_work_steps, compare_work_techniques, identify_key_frames, extract_document_data])]
        """Gemini Function Callingç”¨é–¢æ•°å®šç¾©"""
        return [
            {
                "name": "extract_work_steps",
                "description": "å‹•ç”»ã‹ã‚‰ä½œæ¥­æ‰‹é †ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã™ã‚‹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "work_title": {
                            "type": "string",
                            "description": "ä½œæ¥­ã®åç§°"
                        },
                        "estimated_time": {
                            "type": "number",
                            "description": "æ¨å®šä½œæ¥­æ™‚é–“ï¼ˆåˆ†ï¼‰"
                        },
                        "skill_level": {
                            "type": "string",
                            "enum": ["beginner", "intermediate", "expert"],
                            "description": "å¿…è¦ãªæŠ€èƒ½ãƒ¬ãƒ™ãƒ«"
                        },
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "step_number": {"type": "integer", "description": "æ‰‹é †ç•ªå·"},
                                    "action": {"type": "string", "description": "å®Ÿè¡Œã™ã‚‹å‹•ä½œ"},
                                    "tools_used": {
                                        "type": "array", 
                                        "items": {"type": "string"},
                                        "description": "ä½¿ç”¨ã™ã‚‹å·¥å…·"
                                    },
                                    "duration_seconds": {"type": "number", "description": "æ‰€è¦æ™‚é–“ï¼ˆç§’ï¼‰"},
                                    "safety_notes": {"type": "string", "description": "å®‰å…¨ä¸Šã®æ³¨æ„äº‹é …"},
                                    "quality_points": {"type": "string", "description": "å“è³ªç®¡ç†ã®ãƒã‚¤ãƒ³ãƒˆ"},
                                    "expert_tips": {"type": "string", "description": "ç†Ÿç·´è€…ã®ã‚³ãƒ„"},
                                    "common_mistakes": {"type": "string", "description": "ã‚ˆãã‚ã‚‹å¤±æ•—"}
                                },
                                "required": ["step_number", "action"]
                            }
                        }
                    },
                    "required": ["work_title", "steps"]
                }
            },
            {
                "name": "compare_work_techniques",
                "description": "ç†Ÿç·´è€…ã¨éç†Ÿç·´è€…ã®ä½œæ¥­æŠ€è¡“ã‚’æ¯”è¼ƒåˆ†æã™ã‚‹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "overall_assessment": {
                            "type": "object",
                            "properties": {
                                "efficiency_gap": {"type": "string", "description": "åŠ¹ç‡æ€§ã®å·®"},
                                "safety_gap": {"type": "string", "description": "å®‰å…¨æ€§ã®å·®"},
                                "quality_gap": {"type": "string", "description": "å“è³ªã®å·®"}
                            }
                        },
                        "detailed_differences": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "aspect": {"type": "string", "description": "æ¯”è¼ƒè¦³ç‚¹"},
                                    "expert_approach": {"type": "string", "description": "ç†Ÿç·´è€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"},
                                    "novice_approach": {"type": "string", "description": "éç†Ÿç·´è€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"},
                                    "improvement_suggestion": {"type": "string", "description": "æ”¹å–„ææ¡ˆ"},
                                    "impact_level": {
                                        "type": "string", 
                                        "enum": ["high", "medium", "low"],
                                        "description": "å½±éŸ¿åº¦"
                                    },
                                    "training_priority": {
                                        "type": "integer",
                                        "description": "ç ”ä¿®å„ªå…ˆåº¦ï¼ˆ1-5ï¼‰"
                                    }
                                },
                                "required": ["aspect", "expert_approach", "novice_approach", "improvement_suggestion"]
                            }
                        },
                        "recommended_training": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "skill_area": {"type": "string"},
                                    "training_method": {"type": "string"},
                                    "expected_improvement": {"type": "string"}
                                }
                            }
                        }
                    },
                    "required": ["overall_assessment", "detailed_differences"]
                }
            },
            {
                "name": "extract_document_data",
                "description": "æŠ€è¡“æ–‡æ›¸ã‹ã‚‰ã‚­ãƒ¼æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "document_type": {
                            "type": "string",
                            "enum": ["estimate", "blueprint", "defect_report", "specification", "manual", "other"],
                            "description": "æ–‡æ›¸ã®ç¨®é¡"
                        },
                        "key_information": {
                            "type": "object",
                            "properties": {
                                "title": {"type": "string", "description": "æ–‡æ›¸ã‚¿ã‚¤ãƒˆãƒ«"},
                                "version": {"type": "string", "description": "ãƒãƒ¼ã‚¸ãƒ§ãƒ³"},
                                "date": {"type": "string", "description": "ä½œæˆæ—¥"},
                                "department": {"type": "string", "description": "æ‹…å½“éƒ¨ç½²"}
                            }
                        },
                        "extracted_text": {"type": "string", "description": "OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ"},
                        "terminology": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "term": {"type": "string", "description": "å°‚é–€ç”¨èª"},
                                    "definition": {"type": "string", "description": "å®šç¾©ãƒ»èª¬æ˜"},
                                    "category": {"type": "string", "description": "ã‚«ãƒ†ã‚´ãƒª"}
                                }
                            }
                        },
                        "related_procedures": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "é–¢é€£ã™ã‚‹ä½œæ¥­æ‰‹é †"
                        },
                        "safety_requirements": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å®‰å…¨è¦æ±‚äº‹é …"
                        },
                        "quality_standards": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å“è³ªåŸºæº–"
                        }
                    },
                    "required": ["document_type", "extracted_text"]
                }
            },
            {
                "name": "identify_key_frames",
                "description": "å‹•ç”»ã‹ã‚‰é‡è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ /ç¬é–“ã‚’ç‰¹å®šã™ã‚‹",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "key_moments": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "timestamp": {"type": "number", "description": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆç§’ï¼‰"},
                                    "description": {"type": "string", "description": "ã“ã®ç¬é–“ã®èª¬æ˜"},
                                    "importance": {
                                        "type": "string",
                                        "enum": ["critical", "important", "helpful"],
                                        "description": "é‡è¦åº¦"
                                    },
                                    "category": {
                                        "type": "string",
                                        "enum": ["safety", "quality", "efficiency", "technique"],
                                        "description": "ã‚«ãƒ†ã‚´ãƒª"
                                    },
                                    "annotation_text": {"type": "string", "description": "ç”»åƒã«è¿½åŠ ã™ã¹ãæ³¨é‡ˆ"},
                                    "manual_section": {"type": "string", "description": "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³"}
                                },
                                "required": ["timestamp", "description", "importance"]
                            }
                        }
                    },
                    "required": ["key_moments"]
                }
            }
        ]
    
    async def analyze_expert_novice_comparison(
        self, 
        expert_video_uri: str, 
        novice_video_uri: str, 
        context_docs: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ç†Ÿç·´è€…ãƒ»éç†Ÿç·´è€…å‹•ç”»ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ
        
        Args:
            expert_video_uri: ç†Ÿç·´è€…å‹•ç”»ã®GCS URI
            novice_video_uri: éç†Ÿç·´è€…å‹•ç”»ã®GCS URI
            context_docs: å‚è€ƒæ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            æ¯”è¼ƒåˆ†æçµæœ
        """
        logger.info("ç†Ÿç·´è€…ãƒ»éç†Ÿç·´è€…å‹•ç”»ã®æ¯”è¼ƒåˆ†æã‚’é–‹å§‹")
        
        try:
            # 1. å€‹åˆ¥å‹•ç”»åˆ†æ
            expert_analysis = await self._analyze_single_video(expert_video_uri, "expert")
            novice_analysis = await self._analyze_single_video(novice_video_uri, "novice")
            
            # 2. æ–‡æ›¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
            doc_context = ""
            if context_docs:
                doc_context = await self._process_context_documents(context_docs)
            
            # 3. æ¯”è¼ƒåˆ†æå®Ÿè¡Œ
            comparison_prompt = f"""
è£½é€ æ¥­ã®ä½œæ¥­å‹•ç”»æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

**å‰ç½®ãã®æŒ¨æ‹¶ã‚„èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚ç›´æ¥compare_work_techniquesé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚**

# åˆ†æå¯¾è±¡
## ç†Ÿç·´è€…å‹•ç”»åˆ†æçµæœ:
{json.dumps(expert_analysis, ensure_ascii=False, indent=2)}

## éç†Ÿç·´è€…å‹•ç”»åˆ†æçµæœ:
{json.dumps(novice_analysis, ensure_ascii=False, indent=2)}

# å‚è€ƒè³‡æ–™
{doc_context}

# åˆ†æè¦æ±‚
ä»¥ä¸‹ã®è¦³ç‚¹ã§è©³ç´°ãªæ¯”è¼ƒåˆ†æã‚’è¡Œã„ã€compare_work_techniquesé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

## é‡è¦ãªåˆ†æè¦³ç‚¹:
1. **å‹•ä½œåŠ¹ç‡æ€§ã®é•ã„**: ç„¡é§„ãªå‹•ä½œã€æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹ã€æ™‚é–“åŠ¹ç‡
2. **å®‰å…¨æ€§ã¸ã®é…æ…®ã®å·®**: å®‰å…¨è£…å‚™ã®ä½¿ç”¨ã€å±é™ºäºˆæ¸¬ã€äº‹æ•…é˜²æ­¢è¡Œå‹•
3. **å“è³ªç®¡ç†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„**: ãƒã‚§ãƒƒã‚¯é »åº¦ã€ç²¾åº¦ç¢ºèªæ–¹æ³•ã€å“è³ªåŸºæº–éµå®ˆ
4. **å·¥å…·ä½¿ç”¨æ–¹æ³•ã®å·®ç•°**: æŒã¡æ–¹ã€ä½¿ç”¨é †åºã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹é…æ…®
5. **æ™‚é–“åŠ¹ç‡æ€§ã®æ¯”è¼ƒ**: å„å·¥ç¨‹ã®æ™‚é–“é…åˆ†ã€å…¨ä½“ã®æµã‚Œ
6. **æŠ€è¡“çš„ç¿’ç†Ÿåº¦**: é›£æ˜“åº¦ã®é«˜ã„æŠ€è¡“ã®ç¿’å¾—åº¦ã€å¿œç”¨åŠ›

## åˆ†ææ·±åº¦:
- å…·ä½“çš„ãªè¡Œå‹•ã®é•ã„ã‚’è©³ç´°ã«è¨˜è¿°
- æ”¹å–„ææ¡ˆã¯å®Ÿè£…å¯èƒ½æ€§ã‚‚è€ƒæ…®
- ç ”ä¿®è¨ˆç”»ã¸ã®å…·ä½“çš„ãªçµ„ã¿è¾¼ã¿æ–¹æ³•ã‚‚æç¤º
- æ•°å€¤çš„ãªè©•ä¾¡ã‚‚å¯èƒ½ãªé™ã‚Šå«ã‚ã‚‹

**ã€Œæ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€ãªã©ã®å‰ç½®ãã¯ä¸è¦ã§ã™ã€‚ç›´æ¥compare_work_techniquesé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚**
            """
            
            response = self.model.generate_content(
                comparison_prompt,
                tools=self.tools,
                generation_config={
                    "temperature": 0.1,  # åˆ†æã®ä¸€è²«æ€§ã‚’é‡è¦–
                    "top_p": 0.8,
                    "max_output_tokens": 8192
                },
                safety_settings=self.safety_settings
            )
            
            result = self._parse_function_call_response(response)
            result['expert_analysis'] = expert_analysis
            result['novice_analysis'] = novice_analysis
            
            logger.info("æ¯”è¼ƒåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return result
            
        except Exception as e:
            logger.error(f"æ¯”è¼ƒåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            raise e
    
    async def _analyze_single_video(self, video_uri: str, skill_level: str) -> Dict[str, Any]:
        """
        å˜ä¸€å‹•ç”»ã®è©³ç´°åˆ†æ
        
        Args:
            video_uri: å‹•ç”»ã®GCS URIã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹
            skill_level: ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ï¼ˆexpert/noviceï¼‰
            
        Returns:
            å‹•ç”»åˆ†æçµæœ
        """
        logger.info(f"{skill_level}å‹•ç”»ã®åˆ†æã‚’é–‹å§‹: {video_uri}")
        
        # URIã®ç¨®é¡ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
        if video_uri.startswith('gs://'):
            # GCS URI ã®å ´åˆ
            video_part = Part.from_uri(video_uri, mime_type='video/mp4')
        elif video_uri.startswith('http://') or video_uri.startswith('https://'):
            # HTTP URL ã®å ´åˆ
            video_part = Part.from_uri(video_uri, mime_type='video/mp4')
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦é€ä¿¡
            video_part = await self._load_local_video(video_uri)
        
        skill_level_ja = "ç†Ÿç·´è€…" if skill_level == "expert" else "éç†Ÿç·´è€…"
        
        analysis_prompt = f"""
ã“ã®å‹•ç”»ã¯è£½é€ æ¥­ã«ãŠã‘ã‚‹{skill_level_ja}ã®ä½œæ¥­æ˜ åƒã§ã™ã€‚

**å‰ç½®ãã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚ç›´æ¥extract_work_stepsé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚**

ä»¥ä¸‹ã®è¦³ç‚¹ã§è©³ç´°åˆ†æã‚’è¡Œã„ã€extract_work_stepsé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ï¼š

## åˆ†æé‡ç‚¹é …ç›®:
1. **ä½œæ¥­æ‰‹é †ã®åˆ†è§£ã¨æ™‚ç³»åˆ—åˆ†æ**
   - å„å·¥ç¨‹ã®é–‹å§‹ãƒ»çµ‚äº†æ™‚é–“
   - æ‰‹é †ã®è«–ç†çš„é †åºæ€§
   - ä¸¦è¡Œä½œæ¥­ã®æœ‰ç„¡

2. **ä½¿ç”¨å·¥å…·ã®è­˜åˆ¥ã¨ä½¿ç”¨æ–¹æ³•**
   - å·¥å…·ã®ç¨®é¡ã¨é¸æŠç†ç”±
   - æŒã¡æ–¹ã€æ“ä½œæ–¹æ³•
   - å·¥å…·ã®çŠ¶æ…‹ç¢ºèªè¡Œå‹•

3. **å®‰å…¨æ€§ã«é–¢ã™ã‚‹è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³**
   - ä¿è­·å…·ã®ç€ç”¨çŠ¶æ³
   - å±é™ºäºˆæ¸¬è¡Œå‹•
   - å®‰å…¨ç¢ºèªæ‰‹é †

4. **å“è³ªç®¡ç†ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**
   - æ¸¬å®šãƒ»ç¢ºèªè¡Œå‹•
   - å“è³ªåŸºæº–ã®ç¢ºèªé »åº¦
   - ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ»ä¿®æ­£ãƒ—ãƒ­ã‚»ã‚¹

5. **åŠ¹ç‡æ€§ã¨æŠ€è¡“ãƒ¬ãƒ™ãƒ«**
   - å‹•ä½œã®æ»‘ã‚‰ã‹ã•
   - ç„¡é§„ãªå‹•ä½œã®æœ‰ç„¡
   - æŠ€è¡“çš„ãªç†Ÿç·´åº¦

6. **æ³¨æ„ã™ã¹ãå±é™ºç®‡æ‰€**
   - æ½œåœ¨çš„ãªå±é™ºè¦å› 
   - äº‹æ•…ã«ã¤ãªãŒã‚Šãã†ãªè¡Œå‹•

## {skill_level_ja}ç‰¹æœ‰ã®ç‰¹å¾´:
{skill_level_ja}ç‰¹æœ‰ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã€æŠ€è¡“ãƒ¬ãƒ™ãƒ«ã€å®‰å…¨æ„è­˜ã«ç‰¹ã«æ³¨ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

**ã€Œæ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€ãªã©ã®å‰ç½®ãã¯ä¸è¦ã§ã™ã€‚ç›´æ¥extract_work_stepsé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚**
        """
        
        try:
            logger.info(f"Gemini APIã«å‹•ç”»åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­ - {skill_level}")
            response = self.model.generate_content(
                [video_part, analysis_prompt],
                tools=self.tools,
                generation_config={
                    "temperature": 0.1,
                    "top_p": 0.8,
                    "max_output_tokens": 8192
                },
                safety_settings=self.safety_settings
            )
            logger.info(f"Gemini APIã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ - {skill_level}")
        except Exception as e:
            logger.error(f"Gemini APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´° - video_part: {type(video_part)}, skill_level: {skill_level}")
            raise
        
        result = self._parse_function_call_response(response)
        logger.info(f"{skill_level}å‹•ç”»ã®åˆ†æãŒå®Œäº†")
        return result
    
    async def _load_local_video(self, video_path: str) -> Part:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        
        Args:
            video_path: ãƒ­ãƒ¼ã‚«ãƒ«å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            Part: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        import os
        
        # ãƒ‘ã‚¹ã®æ­£è¦åŒ–ï¼ˆWindowså½¢å¼ã®ãƒ‘ã‚¹å¯¾å¿œï¼‰
        if video_path.startswith('/uploads/') or video_path.startswith('uploads/'):
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’ãƒ•ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›
            current_dir = Path(__file__).parent.parent  # manual_generator directory
            # Windowså½¢å¼ã®ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ­£è¦åŒ–
            clean_path = video_path.replace('\\', '/').lstrip('/')
            full_path = current_dir / clean_path
        else:
            full_path = Path(video_path)
        
        logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {full_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not full_path.exists():
            raise FileNotFoundError(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {full_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆGemini APIã®åˆ¶é™ç¢ºèªï¼‰
        file_size = full_path.stat().st_size
        file_size_mb = file_size / (1024*1024)
        
        # ä¸€æ™‚çš„ã«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’ç·©å’Œï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        max_size = 200 * 1024 * 1024  # 200MBåˆ¶é™
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f}MB")
        
        if file_size > max_size:
            max_size_mb = max_size / (1024*1024)
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…é: {file_size_mb:.1f}MB > {max_size_mb:.0f}MB")
            raise ValueError(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒ{max_size_mb:.0f}MBã‚’è¶…ãˆã¦ã„ã¾ã™: {file_size_mb:.1f}MBã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚")
        
        # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è­¦å‘Šã‚’å‡ºåŠ›
        if file_size_mb > 50:
            logger.warning(f"å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã§ã™: {file_size_mb:.1f}MB - å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        with open(full_path, 'rb') as video_file:
            video_data = video_file.read()

        logger.info(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(video_data)} bytes")

        # Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆbytesãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ï¼‰
        part = Part.from_bytes(data=video_data, mime_type='video/mp4')
        logger.info(f"Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆå®Œäº† - ã‚µã‚¤ã‚º: {len(video_data)} bytes, mime_type: video/mp4")
        return part
    
    async def _process_context_documents(self, doc_paths: List[str]) -> str:
        """
        å‚è€ƒæ–‡æ›¸ã®å‡¦ç†ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            doc_paths: æ–‡æ›¸ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†ã•ã‚ŒãŸæ–‡æ›¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        logger.info(f"{len(doc_paths)}ä»¶ã®å‚è€ƒæ–‡æ›¸ã‚’å‡¦ç†ä¸­")
        
        processed_docs = []
        for doc_path in doc_paths:
            try:
                doc_result = await self.process_document_with_ocr(doc_path, "reference")
                processed_docs.append(doc_result)
            except Exception as e:
                logger.warning(f"æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {doc_path} - {str(e)}")
        
        # æ–‡æ›¸æƒ…å ±ã‚’çµ±åˆ
        context = "## å‚è€ƒè³‡æ–™æƒ…å ±:\n"
        for i, doc in enumerate(processed_docs, 1):
            if 'arguments' in doc and 'extracted_text' in doc['arguments']:
                context += f"### è³‡æ–™{i}:\n{doc['arguments']['extracted_text'][:500]}...\n\n"
        
        return context
    
    async def process_document_with_ocr(self, document_path: str, document_type: str) -> Dict[str, Any]:
        """
        æ–‡æ›¸ã®OCRå‡¦ç†ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        
        Args:
            document_path: æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            document_type: æ–‡æ›¸ã®ç¨®é¡
            
        Returns:
            æ§‹é€ åŒ–ã•ã‚ŒãŸæ–‡æ›¸ãƒ‡ãƒ¼ã‚¿
        """
        logger.info(f"æ–‡æ›¸ã®OCRå‡¦ç†ã‚’é–‹å§‹: {document_path}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
            with open(document_path, 'rb') as f:
                file_data = f.read()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®åˆ¤å®šã¨Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            file_extension = Path(document_path).suffix.lower()
            
            if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                mime_type = f'image/{file_extension[1:]}'
                if file_extension == '.jpg':
                    mime_type = 'image/jpeg'
            elif file_extension == '.pdf':
                mime_type = 'application/pdf'
            else:
                # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã¨ã—ã¦å‡¦ç†ã‚’è©¦è¡Œ
                mime_type = 'image/jpeg'
            
            document_part = Part.from_bytes(data=file_data, mime_type=mime_type)
            
            ocr_prompt = f"""
ã“ã®{document_type}ã®æ–‡æ›¸ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

## OCRãƒ»æ–‡æ›¸è§£æã‚¿ã‚¹ã‚¯:
1. **ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º**: å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«OCRã§èª­ã¿å–ã‚Š
2. **æ–‡æ›¸æ§‹é€ ç†è§£**: éšå±¤ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€è¡¨çµ„ã¿ãªã©ã®æ§‹é€ ã‚’æŠŠæ¡
3. **ã‚­ãƒ¼æƒ…å ±è­˜åˆ¥**: é‡è¦ãªæ•°å€¤ã€ä»•æ§˜ã€æ³¨æ„äº‹é …ã‚’ç‰¹å®š
4. **å°‚é–€ç”¨èªæŠ½å‡º**: æŠ€è¡“ç”¨èªã¨ãã®æ–‡è„ˆã‚’ç†è§£
5. **ä½œæ¥­é–¢é€£æƒ…å ±**: æ‰‹é †ã€å®‰å…¨è¦æ±‚ã€å“è³ªåŸºæº–ã‚’æŠ½å‡º

## ç‰¹ã«æ³¨ç›®ã™ã¹ãè¦ç´ :
- éƒ¨å“ç•ªå·ã€å‹ç•ªã€ä»•æ§˜å€¤
- å®‰å…¨è­¦å‘Šã€æ³¨æ„äº‹é …
- å“è³ªåŸºæº–ã€è¨±å®¹å€¤
- ä½œæ¥­æ‰‹é †ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- å›³è¡¨ã€ã‚°ãƒ©ãƒ•ã®å†…å®¹

extract_document_dataé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã®ç¨®é¡ã«é©ã—ãŸæƒ…å ±ã‚’é‡ç‚¹çš„ã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            """
            
            response = self.model.generate_content(
                [document_part, ocr_prompt],
                tools=self.tools,
                generation_config={
                    "temperature": 0.1,
                    "top_p": 0.8,
                    "max_output_tokens": 8192
                },
                safety_settings=self.safety_settings
            )
            
            result = self._parse_function_call_response(response)
            logger.info(f"æ–‡æ›¸å‡¦ç†ãŒå®Œäº†: {document_path}")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æ›¸å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {document_path} - {str(e)}")
            raise e
    
    async def generate_comprehensive_manual(
        self, 
        analysis_data: Dict[str, Any], 
        output_config: Dict[str, Any]
    ) -> str:
        """
        åŒ…æ‹¬çš„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆ
        
        Args:
            analysis_data: åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
            output_config: å‡ºåŠ›è¨­å®š
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹
        """
        logger.info("åŒ…æ‹¬çš„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆã‚’é–‹å§‹")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        config = {
            "format": "detailed",
            "sections": ["overview", "preparation", "steps", "expert_tips", "safety", "quality", "troubleshooting"],
            "content_length": "normal",
            "writing_style": "formal",
            "language": "ja",
            "include_comparisons": True,
            **output_config
        }
        
        generation_prompt = f"""
è£½é€ æ¥­ä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®è‡ªå‹•ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚

# å…¥åŠ›åˆ†æãƒ‡ãƒ¼ã‚¿
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

# å‡ºåŠ›è¨­å®š
{json.dumps(config, ensure_ascii=False, indent=2)}

# é‡è¦ãªå‡ºåŠ›ãƒ«ãƒ¼ãƒ«
**å‰ç½®ãã®æ–‡ç« ã¯ä¸€åˆ‡æ›¸ã‹ãšã«ã€ç›´æ¥ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹ã‹ã‚‰é–‹å§‹ã—ã¦ãã ã•ã„ã€‚**
ã€Œæ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€ã€Œï½ã‚’åŸºã«ã€ã€Œï½ã—ã¾ã™ã€ãªã©ã®å°å…¥æ–‡ã¯ä¸è¦ã§ã™ã€‚
æœ€åˆã®æ–‡å­—ã‹ã‚‰ã€Œ# ä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€ã¾ãŸã¯ã€Œ## ğŸ“‹ 1. ä½œæ¥­æ¦‚è¦ã€ã§é–‹å§‹ã—ã¦ãã ã•ã„ã€‚

# ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”Ÿæˆè¦æ±‚

ä»¥ä¸‹ã®æ§‹é€ ã§é«˜å“è³ªãªä½œæ¥­ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

## ğŸ“‹ 1. ä½œæ¥­æ¦‚è¦
- ä½œæ¥­ã®ç›®çš„ã¨é‡è¦æ€§
- å¿…è¦ãªæŠ€èƒ½ãƒ¬ãƒ™ãƒ«ã¨å‰æçŸ¥è­˜
- æ¨å®šä½œæ¥­æ™‚é–“ã¨äººå“¡é…ç½®
- å®Œæˆæ™‚ã®å“è³ªåŸºæº–

## ğŸ”§ 2. æº–å‚™å·¥ç¨‹
- å¿…è¦å·¥å…·ä¸€è¦§ï¼ˆè¦æ ¼ãƒ»ä»•æ§˜å«ã‚€ï¼‰
- ææ–™ãƒ»éƒ¨å“ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- å®‰å…¨è£…å‚™ã¨ä¿è­·å…·ã®ç¢ºèª
- ä½œæ¥­ç’°å¢ƒã®æ•´å‚™äº‹é …

## ğŸ“ 3. è©³ç´°ä½œæ¥­æ‰‹é †
- ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ˜ç¢ºãªæŒ‡ç¤º
- å„å·¥ç¨‹ã®åˆ¤æ–­åŸºæº–ã¨å“è³ªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- æ³¨æ„äº‹é …ã¨å®‰å…¨è­¦å‘Š
- å·¥å…·ã®æ­£ã—ã„ä½¿ç”¨æ–¹æ³•

## ğŸ’¡ 4. ç†Ÿç·´è€…ã®ã‚³ãƒ„ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- åŠ¹ç‡åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ
- é«˜å“è³ªã‚’å®Ÿç¾ã™ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
- æ™‚é–“çŸ­ç¸®ã®å·¥å¤«
- ã‚ˆãã‚ã‚‹å¤±æ•—ã¨ãã®å›é¿æ–¹æ³•

## âš ï¸ 5. å®‰å…¨ç®¡ç†
- æ½œåœ¨çš„ãªå±é™ºè¦å› ã®ç‰¹å®š
- äº‹æ•…é˜²æ­¢ã®ãŸã‚ã®å…·ä½“çš„å¯¾ç­–
- ç·Šæ€¥æ™‚ã®å¯¾å¿œæ‰‹é †
- å®‰å…¨ç¢ºèªã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

## âœ… 6. å“è³ªç®¡ç†
- å“è³ªåŸºæº–ã¨åˆ¤å®šæ–¹æ³•
- æ¸¬å®šãƒ»æ¤œæŸ»ã®ãƒã‚¤ãƒ³ãƒˆ
- ä¸è‰¯å“ã®åˆ¤å®šåŸºæº–
- å“è³ªæ”¹å–„ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

## ğŸ”§ 7. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- ã‚ˆãã‚ã‚‹å•é¡Œã¨ãã®ç—‡çŠ¶
- åŸå› åˆ†æã®æ‰‹é †
- å…·ä½“çš„ãªè§£æ±ºæ–¹æ³•
- å°‚é–€éƒ¨ç½²ã¸ã® ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸºæº–

## ç”Ÿæˆæ¡ä»¶:
- æ–‡ä½“: {config['writing_style']}ï¼ˆformal=æ•¬èªãƒ»ä¸å¯§èªã€conversational=ã‚ã‹ã‚Šã‚„ã™ã„å£èª¿ã€technical=æŠ€è¡“çš„ãƒ»ç°¡æ½”ï¼‰
- è©³ç´°åº¦: {config['content_length']}ï¼ˆverbose=éå¸¸ã«è©³ç´°ã€normal=æ¨™æº–ã€concise=ç°¡æ½”ï¼‰
- è¨€èª: {config['language']}
- æ¯”è¼ƒåˆ†æã®æ´»ç”¨: {"ã‚ã‚Š" if config.get('include_comparisons') else "ãªã—"}

## å“è³ªè¦æ±‚:
- å°‚é–€ç”¨èªã¯æ­£ç¢ºã«ä½¿ç”¨ã—ã€å¿…è¦ã«å¿œã˜ã¦èª¬æ˜ã‚’ä½µè¨˜
- å®‰å…¨æ€§ã«é–¢ã‚ã‚‹äº‹é …ã¯ç‰¹ã«å¼·èª¿
- å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªæŒ‡ç¤º
- èª­ã¿æ‰‹ã®ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸé©åˆ‡ãªè©³ç´°åº¦
- è«–ç†çš„ã§ç†è§£ã—ã‚„ã™ã„æ§‹æˆ

**ç¹°ã‚Šè¿”ã—ã¾ã™ãŒã€å‰ç½®ãã®æŒ¨æ‹¶ã‚„èª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚ç›´æ¥ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹ã‹ã‚‰é–‹å§‹ã—ã¦ãã ã•ã„ã€‚**

ç†Ÿç·´è€…ã¨éç†Ÿç·´è€…ã®æ¯”è¼ƒåˆ†æçµæœã‚’æ´»ç”¨ã—ã€å®Ÿè·µçš„ã§æ•™è‚²åŠ¹æœã®é«˜ã„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        """
        
        response = self.model.generate_content(
            generation_prompt,
            generation_config={
                "temperature": 0.3,  # å‰µé€ æ€§ã¨ãƒãƒ©ãƒ³ã‚¹
                "top_p": 0.9,
                "max_output_tokens": 65535  # Gemini 2.5 Proã®æœ€å¤§æ´»ç”¨
            },
            safety_settings=self.safety_settings
        )
        
        logger.info("ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç”ŸæˆãŒå®Œäº†")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å®‰å…¨ã«å–å¾—
        try:
            content = response.text
        except Exception as e:
            logger.warning(f"response.textã®å–å¾—ã«å¤±æ•—ã€ä»£æ›¿æ–¹æ³•ã‚’ä½¿ç”¨: {str(e)}")
            # è¤‡æ•°ãƒ‘ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã®ä»£æ›¿å‡¦ç†
            if (response.candidates and 
                response.candidates[0].content and 
                response.candidates[0].content.parts):
                
                text_parts = []
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                
                if text_parts:
                    content = '\n'.join(text_parts)
                else:
                    raise Exception(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e)}")
            else:
                raise Exception(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e)}")
        
        # å‰ç½®ãæ–‡ç« ã‚’é™¤å»
        content = self._remove_preamble(content)
        
        return content
    
    def _remove_preamble(self, content: str) -> str:
        """
        LLMã®å‰ç½®ãæ–‡ç« ã‚’é™¤å»ã™ã‚‹
        
        Args:
            content: å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            
        Returns:
            å‰ç½®ãæ–‡ç« ã‚’é™¤å»ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        # ã‚ˆãã‚ã‚‹å‰ç½®ãæ–‡ç« ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        preamble_patterns = [
            r'^æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸ.*?(?=\n|$)',
            r'^.*?ã‚’åŸºã«.*?ä½œæˆã—ã¾ã™.*?(?=\n|$)', 
            r'^.*?ã«åŸºã¥ã„ã¦.*?ç”Ÿæˆã—ã¾ã™.*?(?=\n|$)',
            r'^.*?ã«ã¤ã„ã¦.*?èª¬æ˜ã—ã¾ã™.*?(?=\n|$)',
            r'^.*?ã‹ã‚‰.*?ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’.*?(?=\n|$)',
            r'^.*?è©³ç´°ãªåˆ†æçµæœ.*?åŒ…æ‹¬çš„ãª.*?(?=\n|$)',
            r'^.*?ã”æä¾›ã„ãŸã ã„ãŸ.*?(?=\n|$)',
            r'^ä»¥ä¸‹.*?ä½œæˆã„ãŸã—ã¾ã™.*?(?=\n|$)',
            r'^ãã‚Œã§ã¯.*?(?=\n|$)',
            r'^.*?ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ä½œæˆã„ãŸã—ã¾ã™.*?(?=\n|$)'
        ]
        
        import re
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å‰ç½®ãæ–‡ç« ã‚’é™¤å»
        for pattern in preamble_patterns:
            content = re.sub(pattern, '', content, flags=re.MULTILINE | re.DOTALL)
        
        # å…ˆé ­ã®ç©ºè¡Œã‚’é™¤å»
        content = content.lstrip('\n ')
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã§å§‹ã¾ã£ã¦ã„ãªã„å ´åˆã€æœ€åˆã®æœ‰åŠ¹ãªãƒ˜ãƒƒãƒ€ãƒ¼ã¾ã§ã®å†…å®¹ã‚’é™¤å»
        lines = content.split('\n')
        start_index = 0
        
        for i, line in enumerate(lines):
            stripped_line = line.strip()
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã€ç®‡æ¡æ›¸ããŒè¦‹ã¤ã‹ã£ãŸã‚‰ãã“ã‹ã‚‰é–‹å§‹
            if (stripped_line.startswith('#') or 
                stripped_line.startswith('##') or
                re.match(r'^\d+\.', stripped_line) or
                stripped_line.startswith('- ') or
                stripped_line.startswith('* ') or
                stripped_line.startswith('ğŸ“‹') or
                stripped_line.startswith('ğŸ”§') or
                stripped_line.startswith('ğŸ“')):
                start_index = i
                break
        
        if start_index > 0:
            content = '\n'.join(lines[start_index:])
        
        return content.strip()
    
    async def extract_key_frames_with_ai(self, video_uri: str, manual_content: str) -> Dict[str, Any]:
        """
        AIã«ã‚ˆã‚‹é‡è¦ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        
        Args:
            video_uri: å‹•ç”»ã®GCS URI
            manual_content: ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹
            
        Returns:
            ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±
        """
        logger.info("é‡è¦ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚’é–‹å§‹")
        
        video_part = Part.from_uri(video_uri, mime_type='video/mp4')
        
        frame_extraction_prompt = f"""
ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹ã¨å‹•ç”»ã‚’ç…§åˆã—ã€é‡è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ /ç¬é–“ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

# ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹:
{manual_content[:3000]}...  # æœ€åˆã®3000æ–‡å­—

# å®Ÿè¡Œã‚¿ã‚¹ã‚¯:
1. **é‡è¦ç¬é–“ã®ç‰¹å®š**: ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®å„å·¥ç¨‹ã«å¯¾å¿œã™ã‚‹é‡è¦ãªç¬é–“ã‚’å‹•ç”»ã‹ã‚‰ç‰¹å®š
2. **æ•™è‚²ä¾¡å€¤ã®è©•ä¾¡**: å­¦ç¿’åŠ¹æœã®é«˜ã„å ´é¢ã‚’å„ªå…ˆçš„ã«é¸æŠ
3. **å®‰å…¨ãƒ»å“è³ªãƒã‚¤ãƒ³ãƒˆ**: å®‰å…¨æ€§ã‚„å“è³ªã«é–¢ã‚ã‚‹é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®š
4. **ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­è¨ˆ**: å„ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¿…è¦ãªèª¬æ˜ã‚„æ³¨é‡ˆã‚’è¨­è¨ˆ

# ç‰¹ã«é‡è¦ãªç¬é–“:
- ä½œæ¥­é–‹å§‹æ™‚ã®æº–å‚™ç¢ºèª
- å±é™ºã‚’ä¼´ã†å·¥ç¨‹
- å“è³ªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- ç†Ÿç·´æŠ€è¡“ãŒå¿…è¦ãªå ´é¢
- ã‚ˆãã‚ã‚‹å¤±æ•—ãŒèµ·ãã‚„ã™ã„ç¬é–“
- ä½œæ¥­å®Œäº†æ™‚ã®ç¢ºèªäº‹é …

# å‡ºåŠ›è¦æ±‚:
identify_key_framesé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€ä»¥ä¸‹ã‚’å«ã‚€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- æ­£ç¢ºãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
- å„ç¬é–“ã®æ•™è‚²çš„ä¾¡å€¤
- æ¨å¥¨ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹
- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…ã®å¯¾å¿œã‚»ã‚¯ã‚·ãƒ§ãƒ³

æ•™è‚²åŠ¹æœã‚’æœ€å¤§åŒ–ã§ãã‚‹ã‚ˆã†ã€æœ€ã‚‚é‡è¦ãª10-15å€‹ã®ç¬é–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
        """
        
        response = self.model.generate_content(
            [video_part, frame_extraction_prompt],
            tools=self.tools,
            generation_config={
                "temperature": 0.2,
                "top_p": 0.8,
                "max_output_tokens": 8192
            },
            safety_settings=self.safety_settings
        )
        
        result = self._parse_function_call_response(response)
        logger.info("é‡è¦ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºãŒå®Œäº†")
        return result
    
    def _parse_function_call_response(self, response) -> Dict[str, Any]:
        """
        Function Calling ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹
        
        Args:
            response: Geminiã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
            
        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸçµæœ
        """
        try:
            if not response.candidates or not response.candidates[0].content:
                return {
                    "error": "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“",
                    "success": False
                }
            
            content = response.candidates[0].content
            parts = content.parts
            
            result = {
                "success": True,
                "parts": []
            }
            
            # å„ãƒ‘ãƒ¼ãƒˆã‚’å‡¦ç†
            for part in parts:
                if hasattr(part, 'function_call') and part.function_call:
                    # Function Call ãƒ‘ãƒ¼ãƒˆ
                    function_call = part.function_call
                    result["parts"].append({
                        "type": "function_call",
                        "function_name": function_call.name,
                        "arguments": dict(function_call.args)
                    })
                elif hasattr(part, 'text') and part.text:
                    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆ
                    result["parts"].append({
                        "type": "text",
                        "content": part.text
                    })
            
            # ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ã€å˜ä¸€ã®function_callã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯å¾“æ¥ã®å½¢å¼ã‚‚è¿”ã™
            if len(result["parts"]) == 1:
                part = result["parts"][0]
                if part["type"] == "function_call":
                    result.update({
                        "function_name": part["function_name"],
                        "arguments": part["arguments"]
                    })
                elif part["type"] == "text":
                    result["text_response"] = part["content"]
            
            return result
            
        except Exception as e:
            logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "error": str(e),
                "success": False
            }
    
    def get_generation_config(self, task_type: str) -> Dict[str, Any]:
        """
        ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸç”Ÿæˆè¨­å®šã‚’å–å¾—
        
        Args:
            task_type: ã‚¿ã‚¹ã‚¯ã®ç¨®é¡
            
        Returns:
            ç”Ÿæˆè¨­å®š
        """
        configs = {
            "analysis": {
                "temperature": 0.1,
                "top_p": 0.8,
                "max_output_tokens": 8192
            },
            "generation": {
                "temperature": 0.3,
                "top_p": 0.9,
                "max_output_tokens": 65535
            },
            "extraction": {
                "temperature": 0.2,
                "top_p": 0.8,
                "max_output_tokens": 8192
            }
        }
        
        return configs.get(task_type, configs["analysis"])



# Alias for backward compatibility
GeminiService = GeminiUnifiedService
